import os
import time
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Perceptron
from sklearn.svm import LinearSVC, SVC
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, roc_auc_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.activations import relu, selu
from tensorflow.keras.layers import Activation, LeakyReLU, PReLU
from tensorflow.keras.backend import clear_session


X, y = fetch_openml('mnist_784', return_X_y=True)


n_classes = len(np.unique(y))
print(n_classes)


X = np.asarray(X)
y = np.asarray(y)

print(f"X shape: {X.shape}, y shape: {y.shape}")
print(f"Image shape: {X[0].shape}")


print(np.min(X[0]), np.max(X[0]))


print(X.dtype)
print(y.dtype)


X_full = X / 255.0
print(np.min(X_full), np.max(X_full))


y_full = y.astype('int64')
print(y.dtype)


# train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)

# remove a validation set from the training data
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)


print(X_full.shape[0])
print(X_train.shape[0])
print(X_val.shape[0])
print(X_test.shape[0])


perceptron = Perceptron()
perceptron.fit(X_train, y_train)


perceptron.score(X_val, y_val)


reg_perceptron = Perceptron(alpha=0.0001, tol=0.001,
                            max_iter=1000, early_stopping=True,
                            validation_fraction=0.1, n_iter_no_change=5,
                            random_state=42, verbose=0, n_jobs=-1)

perceptron_params = [{'penalty': ['None', 'l1', 'l2', 'elasticnet']}]

grid_search = GridSearchCV(reg_perceptron, perceptron_params,
                           scoring='accuracy', cv=5,
                           return_train_score=True,
                           verbose=3)

grid_search.fit(X_train, y_train)


grid_search.best_params_


model = Sequential()
model.add(Dense(100, activation="relu", input_shape=[784]))
model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])


model.summary()


history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_val, y_val))


def get_tb_dir():
    curr_dir = os.path.join(os.curdir, "tensorboard_logs")
    tb_dir = time.strftime("model_%Y_%m_%d-%H-%M-%S")
    return os.path.join(curr_dir, tb_dir)


tensorboard = TensorBoard(get_tb_dir())


model = Sequential()
model.add(Dense(100, activation="relu", input_shape=[784]))
model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_val, y_val),
                    callbacks=[tensorboard])


# !kill 132185
# %reload_ext tensorboard
# %tensorboard --logdir=./tensorboard_logs --port=6006


def create_lr_model(lr):
    model = Sequential()
    model.add(BatchNormalization(input_shape=[784]))
    model.add(Dense(100, activation="relu"))
    for i in range(20):
        model.add(BatchNormalization())
        model.add(Dense(50, activation="relu"))
    model.add(Dense(10, activation="softmax"))
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer=SGD(lr=lr, momentum=0.9), metrics=["accuracy"])


np.random.seed(42)
lr_model = KerasClassifier(build_fn=create_model, epochs=10)
param_grid = {"lr": [0.001, 0.01, 0.1, 0.2]}

# grid search
lr_grid = GridSearchCV(act_model, param_grid, cv=2, verbose=0)
lr_grid_results = act_grid.fit(X_train, y_train,
                               validation_data=[X_val, y_val],
                               callbacks=[tensorboard, early_stopping],
                               verbose=0)


model = Sequential()
model.add(Dense(100, activation="relu", input_shape=[784]))
for i in range(30):
    model.add(Dense(100, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])
model.summary()


early_stopping = EarlyStopping(patience=10, restore_best_weights=True)


history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_val, y_val),
                    callbacks=[tensorboard, early_stopping])


# Return activation function as layer
def get_activation(activation):
    if (activation == 'relu'):
        return Activation(relu)
    elif (activation == 'selu'):
        return Activation(selu)
    elif (activation == 'leakyrelu'):
        return LeakyReLU()
    elif (activation == 'prelu'):
        return PReLU()
    return False


# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
    # Sequential model
    model = Sequential()
    # Input layer with activation layer
    model.add(Dense(100, input_shape=[784]))
    model.add(get_activation(activation))
    # Hidden layers with activation function
    for i in range(21):
        model.add(Dense(50))
        model.add(get_activation(activation))
    # Output layer
    model.add(Dense(10, activation='sigmoid'))
    # Compile model
    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                  metrics=['accuracy'])
    return model


np.random.seed(42)
act_model = KerasClassifier(build_fn=create_model, epochs=10)
param_grid = {"activation": ['relu', 'selu', 'leakyrelu', 'prelu']}

# grid search
act_grid = GridSearchCV(act_model, param_grid, cv=2, verbose=0)
act_grid_results = act_grid.fit(X_train, y_train,
                                validation_data=[X_val, y_val],
                                callbacks=[tensorboard, early_stopping])


act_grid_results.best_params_


model = Sequential()
model.add(BatchNormalization(input_shape=[784]))
model.add(Dense(100, activation="relu"))
for i in range(20):
    model.add(BatchNormalization())
    model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])
model.summary()


history = model.fit(X_train, y_train, epochs=20,
                    validation_data=[X_val, y_val],
                    callbacks=[tensorboard, early_stopping])


model = Sequential()
model.add(BatchNormalization(input_shape=[784]))
model.add(Dense(100, activation="relu"))
for i in range(20):
    model.add(BatchNormalization())
    model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=SGD(momentum=0.9), metrics=["accuracy"])


history = model.fit(X_train, y_train, epochs=20,
                    validation_data=[X_val, y_val],
                    callbacks=[tensorboard, early_stopping])


adam_model = Sequential()
adam_model.add(Dense(100, activation="relu"))
adam_model.add(BatchNormalization(input_shape=[784]))
for i in range(20):
    adam_model.add(Dense(50, activation="relu"))
    adam_model.add(BatchNormalization())
adam_model.add(Dense(10, activation="softmax"))

adam_model.compile(loss="sparse_categorical_crossentropy",
                   optimizer=Adam(), metrics=["accuracy"])


clear_session()

history = adam_model.fit(X_train, y_train, epochs=20,
                         validation_data=[X_val, y_val],
                         callbacks=[tensorboard, early_stopping])


clear_session()

history = adam_model.fit(X_train, y_train, epochs=20, batch_size=16,
                         validation_data=[X_val, y_val],
                         callbacks=[tensorboard, early_stopping])


drop_model = Sequential()

drop_model.add(BatchNormalization(input_shape=[784]))
drop_model.add(Dense(100, activation="relu"))

for i in range(20):
    drop_model.add(Dense(50, activation="relu"))
    drop_model.add(BatchNormalization())

drop_model.add(Dense(50, activation="relu"))
drop_model.add(Dropout(rate=0.5))

drop_model.add(Dense(10, activation="softmax"))

drop_model.compile(loss="sparse_categorical_crossentropy",
                   optimizer='sgd', metrics=["accuracy"])


clear_session()

history = drop_model.fit(X_train, y_train, epochs=20,
                         validation_data=[X_val, y_val],
                         callbacks=[tensorboard, early_stopping])


# initial cnn model


# cnn with max pooling


# big cnn model


linear_svm = LinearSVC(loss="squared_hinge", dual=False,
                       max_iter=1000, verbose=10)
linear_svm.fit(X_train, y_train)


linear_svm.score(X_val, y_val)


grid_linear_svm = LinearSVC(loss="squared_hinge", dual=False,
                            max_iter=1000)

c_params = {"C": [0.01, 0.1, 1, 10, 10]}

c_gridsearch = GridSearchCV(grid_linear_svm, c_params,
                            cv=2, verbose=3)


c_gridsearch.fit(X_train, y_train)


c_gridsearch.best_params_


c_gridsearch.best_estimator_


c_gridsearch.best_score_


poly_params = {"degree": [2, 3]}
grid_poly = SVC(kernel='poly', max_iter=1000)
poly_gridsearch = GridSearchCV(grid_poly, poly_params, cv=2)


poly_gridsearch.fit(X_train, y_train)


poly_gridsearch.best_params_


poly_gridsearch.best_estimator_


poly_gridsearch.best_score_


rbf_params = {"gamma": [1, 5, 'scale', 'auto']}
grid_rbf = SVC(kernel='rbf', max_iter=5000)
rbf_gridsearch = GridSearchCV(grid_rbf, rbf_params, cv=2)


rbf_gridsearch.fit(X_train, y_train)


rbf_gridsearch.best_params_


rbf_gridsearch.best_estimator_


rbf_gridsearch.best_score_


sigmoid_params = {"gamma": [1, 5, 10, 'scale', 'auto']}
grid_sigmoid = SVC(kernel='sigmoid', max_iter=5000)
sigmoid_gridsearch = GridSearchCV(grid_sigmoid, sigmoid_params, cv=2)


sigmoid_gridsearch.fit(X_train, y_train)


sigmoid_gridsearch.best_params_


sigmoid_gridsearch.best_estimator_


sigmoid_gridsearch.best_score_


svm_gs = [c_gridsearch, poly_gridsearch, rbf_gridsearch, sigmoid_gridsearch]
svms = [gs.best_estimator_ for gs in svm_gs]
svm_preds = [svm.predict(X_test) for svm in svms]


results = pd.DataFrame(columns=["precision_score", "recall_score", "f1_score"])


svm_cms = [confusion_matrix(preds, y_test) for (svm, preds) in zip(svms, svm_preds)]


fig, ax = plt.subplots(figsize=(22, 18))

plt.subplot(221)
fig1 = sns.heatmap(svm_cms[0]/np.sum(svm_cms[0]), annot=True,
                   fmt='.2%', cmap='Blues')
fig1.set_title("Confusion Matrix for Linear SVM")

plt.subplot(222)
fig1 = sns.heatmap(svm_cms[1]/np.sum(svm_cms[1]), annot=True,
                   fmt='.2%', cmap='Blues')
fig1.set_title("Confusion Matrix for Polynomial Kernel SVM")

plt.subplot(223)
fig1 = sns.heatmap(svm_cms[2]/np.sum(svm_cms[2]), annot=True,
                   fmt='.2%', cmap='Blues')
fig1.set_title("Confusion Matrix for RBF Kernel SVM")

plt.subplot(224)
fig1 = sns.heatmap(svm_cms[3]/np.sum(svm_cms[3]), annot=True,
                   fmt='.2%', cmap='Blues')
fig1.set_title("Confusion Matrix for Sigmoid Kernel SVM")


reports = [classification_report(pred, y_test) for pred in svm_preds]


for (report, svm) in zip(reports, ["Linear", "Polynomial", "RBF", "Sigmoid"]):
    print(svm, "SVM:\n", report, "\n")
