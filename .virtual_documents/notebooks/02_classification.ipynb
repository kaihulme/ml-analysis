import os
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Perceptron
from sklearn.svm import LinearSVC, SVC
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.activations import relu, selu
from tensorflow.keras.layers import Activation, LeakyReLU, PReLU
from tensorflow.keras.backend import clear_session


X, y = fetch_openml('mnist_784', return_X_y=True)


n_classes = len(np.unique(y))
print(n_classes)


X = np.asarray(X)
y = np.asarray(y)

print(f"X shape: {X.shape}, y shape: {y.shape}")
print(f"Image shape: {X[0].shape}")


print(np.min(X[0]), np.max(X[0]))


print(X.dtype)
print(y.dtype)


X_full = X / 255.0
print(np.min(X_full), np.max(X_full))


y_full = y.astype('int64')
print(y.dtype)


# train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)

# remove a validation set from the training data
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)


print(X_full.shape[0])
print(X_train.shape[0])
print(X_val.shape[0])
print(X_test.shape[0])


perceptron = Perceptron()
perceptron.fit(X_train, y_train)


perceptron.score(X_val, y_val)


reg_perceptron = Perceptron(alpha=0.0001, tol=0.001,
                            max_iter=1000, early_stopping=True,
                            validation_fraction=0.1, n_iter_no_change=5,
                            random_state=42, verbose=0, n_jobs=-1)

perceptron_params = [{'penalty': ['None', 'l1', 'l2', 'elasticnet']}]

grid_search = GridSearchCV(reg_perceptron, perceptron_params,
                           scoring='accuracy', cv=5,
                           return_train_score=True,
                           verbose=3)

grid_search.fit(X_train, y_train)


grid_search.best_params_


model = Sequential()
model.add(Dense(100, activation="relu", input_shape=[784]))
model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])


model.summary()


history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_val, y_val))


def get_tb_dir():
    curr_dir = os.path.join(os.curdir, "tensorboard_logs")
    tb_dir = time.strftime("model_%Y_%m_%d-%H-%M-%S")
    return os.path.join(curr_dir, tb_dir)


tensorboard = TensorBoard(get_tb_dir())


model = Sequential()
model.add(Dense(100, activation="relu", input_shape=[784]))
model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_val, y_val),
                    callbacks=[tensorboard])


# !kill 132185
# %reload_ext tensorboard
# %tensorboard --logdir=./tensorboard_logs --port=6006


model = Sequential()
model.add(Dense(100, activation="relu", input_shape=[784]))
for i in range(30):
    model.add(Dense(100, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])
model.summary()


early_stopping = EarlyStopping(patience=10, restore_best_weights=True)


history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_val, y_val),
                    callbacks=[tensorboard, early_stopping])


# Function to create model, required for KerasClassifier
def create_init_model(init):
    # Sequential model
    model = Sequential()
    # Input layer with activation layer
    model.add(Dense(100, kernel_input_shape=[784]))
    model.add(get_activation(activation))
    # Hidden layers with activation function
    for i in range(21):
        model.add(Dense(50))
        model.add(get_activation(activation))
    # Output layer
    model.add(Dense(10, activation='sigmoid'))
    # Compile model
    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                  metrics=['accuracy'])
    return model


init_model = KerasClassifier(build_fn=create_init_model, epochs=10)
param_grid = {"init": ['lecun_uniform', 'glorot_normal', 'glorot_uniform',
                       'he_normal', 'he_uniform']}


# Return activation function as layer
def get_activation(activation):
    if (activation == 'relu'):
        return Activation(relu)
    elif (activation == 'selu'):
        return Activation(selu)
    elif (activation == 'leakyrelu'):
        return LeakyReLU()
    elif (activation == 'prelu'):
        return PReLU()
    return False


# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
    # Sequential model
    model = Sequential()
    # Input layer with activation layer
    model.add(Dense(100, input_shape=[784]))
    model.add(get_activation(activation))
    # Hidden layers with activation function
    for i in range(21):
        model.add(Dense(50))
        model.add(get_activation(activation))
    # Output layer
    model.add(Dense(10, activation='sigmoid'))
    # Compile model
    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                  metrics=['accuracy'])
    return model


np.random.seed(42)
act_model = KerasClassifier(build_fn=create_model, epochs=10)
param_grid = {"activation": ['relu', 'selu', 'leakyrelu', 'prelu']}

# grid search
act_grid = RandomizedSearchCV(act_model, param_grid, n_iter=4, cv=2, verbose=0)
act_grid_results = act_grid.fit(X_train, y_train,
                                validation_data=[X_val, y_val],
                                callbacks=[tensorboard, early_stopping],
                                verbose=0)


act_grid_results.best_params_


model = Sequential()
model.add(BatchNormalization(input_shape=[784]))
model.add(Dense(100, activation="relu"))
for i in range(20):
    model.add(BatchNormalization())
    model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer='sgd', metrics=["accuracy"])
model.summary()


history = model.fit(X_train, y_train, epochs=20,
                    validation_data=[X_val, y_val],
                    callbacks=[tensorboard, early_stopping])


model = Sequential()
model.add(BatchNormalization(input_shape=[784]))
model.add(Dense(100, activation="relu"))
for i in range(20):
    model.add(BatchNormalization())
    model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=SGD(momentum=0.9), metrics=["accuracy"])


history = model.fit(X_train, y_train, epochs=20,
                    validation_data=[X_val, y_val],
                    callbacks=[tensorboard, early_stopping])


adam_model = Sequential()
adam_model.add(Dense(100, activation="relu"))
adam_model.add(BatchNormalization(input_shape=[784]))
for i in range(20):
    adam_model.add(Dense(50, activation="relu"))
    adam_model.add(BatchNormalization())
adam_model.add(Dense(10, activation="softmax"))

adam_model.compile(loss="sparse_categorical_crossentropy",
                   optimizer=Adam(), metrics=["accuracy"])


clear_session()

history = adam_model.fit(X_train, y_train, epochs=20,
                         validation_data=[X_val, y_val],
                         callbacks=[tensorboard, early_stopping])


clear_session()

history = adam_model.fit(X_train, y_train, epochs=20, batch_size=16,
                         validation_data=[X_val, y_val],
                         callbacks=[tensorboard, early_stopping])


def create_lr_model(lr):
    model = Sequential()
    model.add(BatchNormalization(input_shape=[784]))
    model.add(Dense(100, activation="relu"))
    for i in range(20):
        model.add(BatchNormalization())
        model.add(Dense(50, activation="relu"))
    model.add(Dense(10, activation="softmax"))
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer=SGD(lr=lr, momentum=0.9), metrics=["accuracy"])


np.random.seed(42)
lr_model = KerasClassifier(build_fn=create_model, epochs=10)
param_grid = {"lr": []}

# grid search
lr_grid = GridSearchCV(act_model, param_grid, cv=2, verbose=0)
lr_grid_results = act_grid.fit(X_train, y_train,
                               validation_data=[X_val, y_val],
                               callbacks=[tensorboard, early_stopping],
                               verbose=0)


drop_model = Sequential()

drop_model.add(BatchNormalization(input_shape=[784]))
drop_model.add(Dense(100, activation="relu"))

for i in range(20):
    drop_model.add(Dense(50, activation="relu"))
    drop_model.add(BatchNormalization())

drop_model.add(Dense(50, activation="relu"))
drop_model.add(Dropout(rate=0.5))

drop_model.add(Dense(10, activation="softmax"))

drop_model.compile(loss="sparse_categorical_crossentropy",
                   optimizer='sgd', metrics=["accuracy"])


clear_session()

history = drop_model.fit(X_train, y_train, epochs=20,
                         validation_data=[X_val, y_val],
                         callbacks=[tensorboard, early_stopping])


# get MNIST data and prepare for model
X, y = fetch_openml('mnist_784', return_X_y=True)
X = np.asarray(X)
y = np.asarray(y)


# normalise and scale
X_full = X / 255.0
X_full = StandardScaler().fit_transform(X_full)

# convert type to int
y_full = y.astype('int64')

# reduce number of features
# X_reduced = PCA(n_components=392).fit_transform(X_full)


# create a small training set for faster training with a test and val set
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_full,
                                                    test_size=0.7)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,
                                                  test_size=0.1)


X_train.shape


plt.imshow(X_full[0].reshape((28,28)), cmap='gray')


linear_svm = LinearSVC(loss="squared_hinge", dual=False,
                       max_iter=1000, verbose=10)
linear_svm.fit(X_train, y_train)


score = linear_svm.score(X_val, y_val)
print(score)


grid_linear_svm = LinearSVC(loss="squared_hinge", dual=False,
                            max_iter=5000)

c_params = {"C": [0.001, 0.01, 0.1, 1, 10, 100]}

c_gridsearch = GridSearchCV(grid_linear_svm, c_params,
                            cv=3, verbose=3)


c_gridsearch.fit(X_train, y_train)


c_gridsearch.best_params_


c_gridsearch.best_estimator_


c_gridsearch.best_score_


poly_params = {"degree": [2, 3],
               "C": [0.1, 1, 10]}

grid_poly = SVC(kernel='poly', coef0=1, max_iter=5000)

poly_gridsearch = GridSearchCV(grid_poly, poly_params, cv=3)


poly_gridsearch.fit(X_train, y_train)


poly_gridsearch.best_params_


poly_gridsearch.best_estimator_


poly_gridsearch.best_score_


rbf_params = {"gamma": [1, 5, 10, 'scale', 'auto'],
              "C": [0.1, 1, 10]}

grid_rbf = SVC(kernel='rbf', dual=False, max_iter=5000)

rbf_gridsearch = GridSearchCV(grid_rbf, rbf_params,
                              cv=3, versbose=3)


rbf_gridsearch.best_params_


rbf_gridsearch.best_estimator_


rbf_gridsearch.best_score_


sigmoid_params = {"gamma": [1, 5, 10, 'scale', 'auto'],
                  "C": [0.1, 1, 10]}

grid_sigmoid = SVC(kernel='poly', coef0=1, max_iter=5000)

sigmoid_gridsearch = GridSearchCV(grid_sigmoid, sigmoid_params, cv=3)


sigmoid_gridsearch.best_params_


sigmoid_gridsearch.best_estimator_


sigmoid_gridsearch.best_score_
