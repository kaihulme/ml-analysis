import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, KernelPCA, FastICA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score


X, y = fetch_openml('mnist_784', return_X_y=True)


X = np.array(X).astype('int')
y = np.array(y).astype('int')

print(f"X shape: {X.shape}, y shape: {y.shape}")
print(f"Image shape: {X[0].shape}")


# train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# remove a validation set from the training data
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)


print(X.shape)
print(X_train.shape)
print(X_val.shape)
print(X_test.shape)


pca = PCA()
pca.fit(X_train)


print(pca.components_)
pca.components_.shape


explained_var_ratio = pca.explained_variance_ratio_
print(explained_var_ratio[:20], "...")


top_var = explained_var_ratio[:10]
print(top_var)
print(f"Variance for top 10 principle components: {top_var.sum():2f}")


cumsum_var_ratio = np.cumsum(explained_var_ratio)
xs = np.arange(len(cumsum_var_ratio))

# beautify this!
fig, ax = plt.subplots(figsize=(12, 8))
ax.set_title("Cummulative variance contribution of adding each PCA component")
ax.set_xlabel("PCA component (decreasing variance)")
ax.set_ylabel("Cummulative variance contribution")
ax.grid()
ax.set_xticks(np.arange(0, len(cumsum_var_ratio), 25))
ax.set_yticks(np.arange(0, 1.1, 0.1))
ax.set_xlim(xmin=0, xmax=300)
ax.set_ylim(ymin=0, ymax=1.1)
ax.plot(xs, cumsum_var_ratio)
plt.show()


pca_50 = PCA(n_components=50)
deconstructed_50 = pca_50.fit_transform(X_train)
reconstructed_50 = pca_50.inverse_transform(deconstructed_50)
reconstructed_50.shape


def plot_reconstructed(plot_imgs, plot_reconstructed_imgs, num=5):
    fig = plt.figure(figsize=(12, 8))
    # get random indices
    indices = np.random.choice(len(imgs), num)
    # show (num) test iamges
    for i, ind in enumerate(indices):
        ax = fig.add_subplot(1, num+1, i+1)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        ax.imshow(imgs[ind], cmap='gray')
    # show (num) reconstructed images
    for i, ind in enumerate(indices):
        ax = fig.add_subplot(2, num+1, i+num+2)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        ax.imshow(reconstructed_imgs[ind], cmap='gray')


imgs = X_train.reshape(-1, 28, 28)
reconstructed_imgs = reconstructed_50.reshape(-1, 28, 28)

print("50 Components (~85% variance):")
plot_reconstructed(imgs, reconstructed_imgs)


def pca_reduce_img(X, n):
    pca = PCA(n_components=n)
    deconstructed = pca.fit_transform(X)
    reconstructed = pca.inverse_transform(deconstructed)
    return reconstructed.reshape(-1, 28, 28)


reconstructed_25 = pca_reduce_img(X_train, 25)
reconstructed_10 = pca_reduce_img(X_train, 10)
reconstructed_5 = pca_reduce_img(X_train, 5)


print("25 Componenets (~70% variance):")
plot_reconstructed(imgs, reconstructed_25)
plt.show()

print("10 Componenets (~50% variance):")
plot_reconstructed(imgs, reconstructed_10)
plt.show()

print("10 Componenets (~30% variance):")
plot_reconstructed(imgs, reconstructed_5)
plt.show()


pca_2 = PCA(n_components=2)
X_train_2 = pca_2.fit_transform(X_train)

X_train_2.shape


def plt_2d_mnist(X, y, title):
    xs, ys = X[:, 0], X[:, 1]
    c = np.array(y).astype('int')
    fix, ax = plt.subplots(figsize=(12, 8))
    ax.set_title(title)
    ax.scatter(xs, ys, c=c, cmap="tab10", s=1, alpha=0.5)
    plt.show()


plt_2d_mnist(X_train_2, y_train, "MNIST Dataset Reduce to 2 Dimensions with PCA")


fica = FastICA(n_components=2, max_iter=1000)
X_fica_2 = fica.fit_transform(X_train_2)

plt_2d_mnist(X_fica_2, y_train, "MNIST Reduced to 2 Dimensions with PCA and Seperated with Fast ICA")


rbf_pca = KernelPCA(n_components=2, kernel='linear')

samples = np.random.choice(len(X_train), 1000)
Xsamples = X_train[samples]
ysamples = y_train[samples]
samples_reduced = rbf_pca.fit_transform(Xsamples)

xs, ys = samples_reduced[:, 0], samples_reduced[:, 1]
c = np.array(ysamples).astype('int')
fix, ax = plt.subplots(figsize=(12, 8))
ax.set_title("MNIST Sample Reduced to 2 Dimensions with Kernel PCA with RBF Kernel")
ax.scatter(xs, ys, c=c, cmap="tab10", s=50, alpha=1)
plt.show()


pca_20 = PCA(n_components=20)
X_train_20 = pca_20.fit_transform(X_train)


tsne = TSNE()
X_train_tsne_2 = tsne.fit_transform(X_train_20)


plt_2d_mnist(X_train_tsne_2, y_train, "MNIST Dataset Reduced to 20 Dimensions with PCA then to 2 with TSNE")


kmeans = KMeans(n_clusters=10, n_init=100)
kmeans.fit(X_train_2)

# centroids = kmeans.cluster_centers_
print(kmeans.cluster_centers_)


def plot_veronoi(X, y, kmeans):
    """
    Plot veronoi diagram with data given KMeans centroids
    """
    xs, ys = X[:, 0], X[:, 1]
    x_min, x_max = xs.min()-1, xs.max()+1
    y_min, y_max = ys.min()-1, ys.max()+1
    c = np.array(y).astype('int')
    centroids = kmeans.cluster_centers_

    # generate veronoi boundaries for plotting
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000),
                         np.linspace(y_min, y_max, 1000))
    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # plot veronoi
    plt.figure(figsize=(12, 8))
    plt.imshow(Z, interpolation='nearest',
               extent=(xx.min(), xx.max(), yy.min(), yy.max()),
               cmap="Pastel1",
               aspect='auto',
               origin='lower')
    plt.contour(Z, extent=(x_min, x_max, y_min, y_max),
                linewidths=1, colors='grey')
    # plot data and centroids
    plt.scatter(xs, ys, s=0.5, alpha=1, color='black')
    plt.scatter(centroids[:, 0], centroids[:, 1], marker='o', color='brown',
                s=50, linewidths=2, zorder=10)
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    plt.axis("off")
    plt.show()


indices = np.random.randint(0, len(y_train), 20000)
sample_X = X_train_2[indices]
sample_y = y_train[indices]

plot_veronoi(sample_X, sample_y, kmeans)


silhouette_score(X_train_2, kmeans.labels_)


score = adjusted_rand_score(y_train, kmeans.labels_)
print(score)


ks = range(20)
kmeans_k = [KMeans(n_clusters=k+1).fit(X_train_2) for k in ks]


inertia_k = [km.inertia_ for km in kmeans_k]


fig, ax = plt.subplots(figsize=(12, 8))
ax.set_title("KMeans Cluster Count Affect on Inertia")
ax.set_xlabel("KMeans clusters")
ax.set_ylabel("Intertia")
ax.set_xticks(ks)
ax.plot(ks, inertia_k)
plt.show


# silhouette_k = [silhouette_score(X_train_2, km.labels_) for km in kmeans_k]
print(kmeans_k[0].labels_.shape)
print(X_train_2.shape)
silhouette_sc = silhouette_score(X_train_2, kmeans_k[0].labels_)


fig, ax = plt.subplots(figsize=(12, 8))
ax.set_title("KMeans Cluster Count Affect on Silhouette Score")
ax.set_xlabel("KMeans clusters")
ax.set_ylabel("Silhouette score")
ax.set_xticks(ks)
ax.plot(ks, silhouette_k)
plt.show


dimensions = np.logspace(0, 8, 9, base=2).astype('int') + 1
print(dimensions)


X_train_d = [PCA(n_components=d).fit_transform(X_train) for d in dimensions]


kmeans_d = [KMeans(n_clusters=10).fit(X) for X in X_train_d]


silhouette_d = [silhouette_score(X, km.labels_) for (X, km) in zip(X_train_d, kmeans_d)]


print(silhouette_d)


scores_d = [adjusted_rand_score(y_train, km.labels_) for km in kmeans_d]


print(scores_d)


results = np.array([dimensions, silhouette_d, scores_d])
df = pd.DataFrame(results.T, columns=['Dimensions', 'Silhouette score', 'Adj.rand score'])
print(df)

fig1_df = df[['Dimensions', 'Silhouette score', 'Adj.rand score']]
fig1_df = pd.melt(fig1_df, ['Dimensions'], var_name="",
                  value_name="Score")

fig, ax = plt.subplots(figsize=(18, 8))
plt.subplot(121)
fig1 = sns.lineplot(data=fig1_df, x='Dimensions', y='Score', hue='')
fig1.set(xlim=[1, 250], ylim=[0.05, 0.45])
fig1.set_title("Affect of kmeans clustering in various dimensional space")


#
