import numpy as np
import pandas as pd
import pymc3 as pm
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, StackingRegressor
from california_data_pipeline import load_train_test
from bayesian_linear_regressor import BayesianLinearRegression


X_train, X_test, y_train, y_test = load_train_test()


X_train.head()


y_train.head()


tree_reg = DecisionTreeRegressor()
tree_reg.fit(X_train, y_train)


# output R2, MSE and RMSE regressor
def reg_metrics(reg, reg_name, X, y):
    reg_preds = reg.predict(X)
    reg_r2 = r2_score(y, reg_preds)
    reg_mse = mean_squared_error(y, reg_preds)
    reg_rmse = mean_squared_error(y, reg_preds, squared=False)
    print(f"{reg_name} regression R2:   {reg_r2:.4f}")
    print(f"{reg_name} regression MSE:  {reg_mse:.4f}")
    print(f"{reg_name} regression RMSE: {reg_rmse:.4f}")


reg_metrics(tree_reg, "decision tree", X_test, y_test)


forest_reg = RandomForestRegressor(n_estimators=10)
forest_reg.fit(X_train, np.ravel(y_train))


reg_metrics(forest_reg, "random forest", X_test, y_test)


forest_reg = RandomForestRegressor(n_estimators=100)
forest_reg.fit(X_train, np.ravel(y_train))
reg_metrics(forest_reg, "random forest", X_test, y_test)


# get score for training set


forest_reg = RandomForestRegressor(n_estimators=1000, max_depth=10, n_jobs=-1)
forest_reg.fit(X_train, np.ravel(y_train))
reg_metrics(forest_reg, "random forest", X_test, y_test)


n_estimators = np.arange(100, 1000)
max_features = ['auto', 'sqrt', 'log2']
max_depth = np.arange(3, 20)
min_samples_split = np.arange(1, 100)
min_samples_leaf = np.arange(1, 100)
bootstrap = [True, False]

forest_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

forest_reg = RandomForestRegressor()

forest_randomcv = RandomizedSearchCV(forest_reg, forest_grid, n_iter=50, cv=2)


forest_randomcv.fit(X_train, np.ravel(y_train))


from sklearn.externals import joblib
joblib.dump(forest_randomcv, 'forest_randomcv.pkl')


# loop training optimal model with varying number of trees
# record trianing time for each model
# plot training time (number of trees) vs oob error


extra_reg = ExtraTreesRegressor()
extra_reg.fit(X_train, y_train)


estimators = [('forest_reg0', RandomForestRegressor()),
              ('forest_reg1', RandomForestRegressor()),
              ('forest_reg2', RandomForestRegressor())]

blender = DecisionTreeRegressor()

stacking_bayesian_forest_reg = StackingRegressor(estimators=estimators,
                                                 final_estimator=blender)


# from bayesian_linear_regressor import BayesianLinearRegression
# bayesian_reg = BayesianLinearRegression()


forest_estimators = [('forest_reg0', RandomForestRegressor()),
                     ('forest_reg1', RandomForestRegressor()),
                     ('forest_reg2', RandomForestRegressor())]

bayesian_estimators = [('bayesian_reg0', BayesianLinearRegression())
                       ('bayesian_reg1', BayesianLinearRegression()),
                       ('bayesian_reg2', BayesianLinearRegression())]

blender = DecisionTreeRegressor()

internal_stack = StackingRegressor(estimators=bayesian_estimators,
                                   final_estimator=blender)

bayesian_forest_stack = StackingRegressor(estimators=forest_estimators,
                                          final_estimator=internal_stack)
