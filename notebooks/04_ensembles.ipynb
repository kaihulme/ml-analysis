{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from california_data_pipeline import load_train_test\n",
    "from bayesian_linear_regressor import BayesianLinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisdom of the crowd\n",
    "\n",
    "If you had a large group of people all take an estimate on how many marbles were in a jar you would get a wide range of answers. An expert on marble guessing may be in the top 10% of closest guesses, but they may still be far off.\n",
    "\n",
    "However if you had access to all these guesses and simply took the median of them all, you would likely be considered better than the experts.\n",
    "\n",
    "This phenomenon is known as the wisdom of the crowd; where the average guess often better than average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods\n",
    "\n",
    "Take the above example, but replace marble counters with machine learning models. The predictions of these models can be combined, each taking votes for classification or the results being averaged for regression. This ensemble of models will also outperform its individual components.\n",
    "\n",
    "A weak learner is a classifier which just better than average. If you created an ensemble of classifiers each with 51% accuracy, given enough weak learners, the ensemble model can perform very well becoming a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and pasting\n",
    "\n",
    "One way to create an ensemble is to use the same model on multiple subsets of the data. The individual results of each model are then combined together to get a results which is usually better than the best performing individual model.\n",
    "\n",
    "When samples are taken out of the dataset and not replaced this is known as pasting, when they are replaced bagging is being performed. Pasting means each model can learn individual patterns as there is no shared data, but a lot of data is needed for this to work well as you are affectively dividing the size of the dataset each model will learn on by the number of models in the ensemble.\n",
    "\n",
    "One benefit of bagging is that each of the classifiers may not see all of the data point in the training set even though data is being replaced. These unseen data points are known as out-of-bag samples and can be used to validate the models without the need for a seperate validation set which means more data can be used to train the model.\n",
    "\n",
    "Another method of bagging involves selecting a random number of features to use for that round of training. The idea being that removing some of the features during training rounds reduces an over-reliability on a few features and makes the model learn a more generalised pattern from the data. This is very similar to how dropout can help reduce overfitting in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Random Forest\n",
    "\n",
    "### California housing data\n",
    "\n",
    "We shall be using the California housing dataset we used with Bayesian regression.\n",
    "\n",
    "As we have already performed our analysis on the data and selected and transformed our features we shall replicate the same data pipeline from the previous notebook.\n",
    "\n",
    "To do this I have implemented a transformation pipeline module which gets the California housing data, transforms and creates features as described previously and then returns the train and test data and real values. The function has optional parameters for test size and transformation type, see module for info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>AveBedrmsPerRoom</th>\n",
       "      <th>AveAddRooms</th>\n",
       "      <th>EstHouses</th>\n",
       "      <th>DistToTown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>-1.114001</td>\n",
       "      <td>-0.813383</td>\n",
       "      <td>1.616400</td>\n",
       "      <td>-1.473897</td>\n",
       "      <td>0.796013</td>\n",
       "      <td>-0.592870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143</th>\n",
       "      <td>-0.914646</td>\n",
       "      <td>1.260378</td>\n",
       "      <td>-0.905558</td>\n",
       "      <td>1.550903</td>\n",
       "      <td>-1.113609</td>\n",
       "      <td>0.497750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9669</th>\n",
       "      <td>-0.258580</td>\n",
       "      <td>2.651988</td>\n",
       "      <td>1.785041</td>\n",
       "      <td>-2.053961</td>\n",
       "      <td>-1.649685</td>\n",
       "      <td>-1.150917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>-0.534559</td>\n",
       "      <td>-0.116728</td>\n",
       "      <td>0.497786</td>\n",
       "      <td>-0.324815</td>\n",
       "      <td>0.074911</td>\n",
       "      <td>2.346136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17756</th>\n",
       "      <td>-0.165095</td>\n",
       "      <td>-0.325182</td>\n",
       "      <td>-0.864095</td>\n",
       "      <td>0.789426</td>\n",
       "      <td>0.039902</td>\n",
       "      <td>1.489587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MedInc  AveOccup  AveBedrmsPerRoom  AveAddRooms  EstHouses  DistToTown\n",
       "15536 -1.114001 -0.813383          1.616400    -1.473897   0.796013   -0.592870\n",
       "12143 -0.914646  1.260378         -0.905558     1.550903  -1.113609    0.497750\n",
       "9669  -0.258580  2.651988          1.785041    -2.053961  -1.649685   -1.150917\n",
       "3005  -0.534559 -0.116728          0.497786    -0.324815   0.074911    2.346136\n",
       "17756 -0.165095 -0.325182         -0.864095     0.789426   0.039902    1.489587"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>0.253606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143</th>\n",
       "      <td>-1.014571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9669</th>\n",
       "      <td>-0.245760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>-1.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17756</th>\n",
       "      <td>-0.792733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedHouseVal\n",
       "15536     0.253606\n",
       "12143    -1.014571\n",
       "9669     -0.245760\n",
       "3005     -1.446104\n",
       "17756    -0.792733"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisions trees\n",
    "\n",
    "A decision tree performs classification or regression based on a flow-chart like graph, where an input to the tree starts at the root and makes its way through the tree based on conditions for each feature of the data point at each node of the tree. At the bottom of the tree is the output prediction for the input features. This is best demontsrated with a diagram \n",
    "\n",
    ">>> INSERT DECISION TREE\n",
    "\n",
    "Decision tree models are trained through supervised learning. The tree finds the optimal way to split the training data at each level, an example might be splitting the tree for houses that are less than 10km from LA and those which are not. The combination of these feature splits can result in very complex models being learned.\n",
    "\n",
    "We shall train a simple decision tree regressor with the preprocessed California housing data we just loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make predictions of the test set values and get scores for R2, MSE, RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output R2, MSE and RMSE regressor\n",
    "def reg_metrics(reg, reg_name, X, y):\n",
    "    reg_preds = reg.predict(X)\n",
    "    reg_r2 = r2_score(y, reg_preds)\n",
    "    reg_mse = mean_squared_error(y, reg_preds)\n",
    "    reg_rmse = mean_squared_error(y, reg_preds, squared=False)\n",
    "    print(f\"{reg_name} regression R2:   {reg_r2:.4f}\")\n",
    "    print(f\"{reg_name} regression MSE:  {reg_mse:.4f}\")\n",
    "    print(f\"{reg_name} regression RMSE: {reg_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree regression R2:   0.4151\n",
      "decision tree regression MSE:  0.5978\n",
      "decision tree regression RMSE: 0.7732\n"
     ]
    }
   ],
   "source": [
    "reg_metrics(tree_reg, \"decision tree\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have used the same data and the same preprocessing (albeit with different random test train split), we can make reasonably accurate comparisons between each of the models we have trained on the data.\n",
    "\n",
    "Looking at the metrics above we can see our decision tree has done quite well, although not as good of a fit as our Bayesian linear regression model.\n",
    "\n",
    "Lets see how we can apply ensemble methods to produce even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "A random forest is an ensemble of decision trees. Using bagging, a number of decision trees are trained together on sub-sets of the training data. The results from each tree are the agregated, producing a result far better than each of its parts.\n",
    "\n",
    "Random forests also implement the random feature selection mentioned previously. Instead of splitting the tree with the best feature, it takes a subset of the features and finds the best split in that. This allows for more features to be utilised by the model and deeper patterns to be learned over non-randomising the feature sets.\n",
    "\n",
    "As above we shall train on the test data and analyse the results, but this time with an ensemble of 10 decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators=10)\n",
    "forest_reg.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest regression R2:   0.6714\n",
      "random forest regression MSE:  0.3358\n",
      "random forest regression RMSE: 0.5795\n"
     ]
    }
   ],
   "source": [
    "reg_metrics(forest_reg, \"random forest\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 10 trees, we have managed to increase R2 and decrease both MSE and RMSE significantly.\n",
    "\n",
    "Lets try increasing the number of estimators to further benefit from the windom of the crowds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest regression R2:   0.6994\n",
      "random forest regression MSE:  0.3073\n",
      "random forest regression RMSE: 0.5543\n"
     ]
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators=100)\n",
    "forest_reg.fit(X_train, np.ravel(y_train))\n",
    "reg_metrics(forest_reg, \"random forest\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how well it fit to the data it was tested on and see if we are overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the model has overfit and not generalised to the test set as well as it could have. Increasing the number of estimators and constraining the tree depth should force the ensemble to generalise more as it learns more important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest regression R2:   0.7000\n",
      "random forest regression MSE:  0.3066\n",
      "random forest regression RMSE: 0.5538\n"
     ]
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators=1000, max_depth=10, n_jobs=-1)\n",
    "forest_reg.fit(X_train, np.ravel(y_train))\n",
    "reg_metrics(forest_reg, \"random forest\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected constraining each model and forcing each to learn stronger patterns helps the ensemble generalise to the \n",
    "\n",
    "We shall run a random hyper parameter search on some different parameters to get a better idea of optimal tree constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest default parameters:\n",
    "- n_estimators=100,\n",
    "- max_depth=Non\n",
    "- min_samples_split=2\n",
    "- min_samples_leaf=1\n",
    "- min_weight_fraction_leaf=0.0\n",
    "- max_features='auto'\n",
    "- max_leaf_nodes=None\n",
    "- min_impurity_decrease=0.0\n",
    "- min_impurity_split=None\n",
    "- bootstrap=True\n",
    "- ccp_alpha=0.0\n",
    "- max_samples=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = np.arange(100, 5000)\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = np.arange(2, 100)\n",
    "min_samples_split = np.arange(1, 100)\n",
    "min_samples_leaf = np.arange(1, 100)\n",
    "bootstrap = [True, False]\n",
    "\n",
    "forest_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "forest_randomcv = RandomizedSearchCV(forest_reg, forest_grid, n_iter=100, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_randomcv.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(forest_randomcv, 'forest_randomcv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- discuss results from search\n",
    "- decide on final parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting branches and features\n",
    "\n",
    "As discussed above limiting the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees in a forest\n",
    "\n",
    "- Plot the trade off between training time and performance\n",
    "- What is the optimal (wrt. training time and accuracy) number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop training optimal model with varying number of trees\n",
    "# record trianing time for each model\n",
    "# plot training time (number of trees) vs oob error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely randomised trees\n",
    "\n",
    "Also called extra trees, extremely randomised trees are an additional extension to the random forest. Where random forests select the best split of a random subset of features, extra trees further increase randomness by selecting a random threshold for each feature.\n",
    "\n",
    "This can produce models which learn more generalised patterns, but their main benifit is they have a much lower computational complexity than random forests.\n",
    "\n",
    "Lets create an extra tree regressor and see how it performs on the training data.\n",
    "\n",
    "- Additional check of extra trees vs random forest\n",
    "    - use opt forest params for both\n",
    "    - or quick grid search for extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_reg = ExtraTreesRegressor()\n",
    "extra_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss results compared to random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a forest\n",
    "\n",
    "- Easier to look at a decision tree but still too large to comprehend\n",
    "    - nothing to learn from looking at the tree that the model hasnt learnt already\n",
    "- Decision trees are weak learners\n",
    "    - small change in input can lead to different trees\n",
    "    - look at .feature_importance_ for a few subsets\n",
    "- Random forests are much stronger learners\n",
    "    - look at .feature_importance_ for same subsets\n",
    "    - shouldnt change as much as decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Stacking takes the idea of ensemble learning further by replacing the simple model agrigation methods such as voting and averaging with another model which is trained by each model's votes. Each model in the prediction ensemble gives its results to a blending model which outputs a final result.\n",
    "\n",
    ">>> Stacking diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking random forests\n",
    "\n",
    "- Stack random forests together\n",
    "- Compare to optimal random forest\n",
    "- Create ensemble of random forests and take mean of results\n",
    "- Compare to stacked and normal random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer stacking\n",
    "\n",
    "The idea of stacking can be extended further, allowing multiple layers of stacked models on-top of eachother. The layers of models can also be different types of models themselves. \n",
    "\n",
    "I shall use this method to stack together a random forest regressor and a bayesian linear regressor. The idea being that the two varied approaches to regression come together for a stronger solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian linear regression\n",
    "\n",
    "In the previous task I created a Scikit-learn wrapper for the PyMC3 baysian linear regression model I created for the California housing data. I shall use this in my stacking ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_linear_regressor import BayesianLinearRegression\n",
    "\n",
    "bayesian_reg = BayesianLinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: as the bayesian linear regression model produces a probability distribution rather than a single estimate, the predict method of the class predicts using the mean of the samples from the distribution. This means I can return a R2 score allowing iheritance of Scikit-learn `base.RegressorMixin` and stacking with `StackingRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking tasks\n",
    "\n",
    "Bayesian linear regression and decision trees are two very different approaches to regression.  Ensemble methods can exploit such diversity between different methods to improve performance.  So now you will try combining the random forest  and  Bayesian  linear  regression  using stacking.\n",
    "\n",
    "Scikit-learn  includes the  StackingRegressor  class  to  help  you  with  this.   In  the  report,  explain the stacking approach and describe your results,  making sure to cover the following points:\n",
    "\n",
    "1. When does stacking improve performance over the individual models (e.g. try stacking with a random forest with 'maxdepth=10' and 'nestimators=10')?\n",
    "2. What happens if we just take the mean prediction from our base models instead?\n",
    "3. Use a DecisionTreeRegressor as the final estimator and visualise the tree to understand what stacking is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of random forest and stacking\n",
    "\n",
    "- Comparison of random forest and stacking methods\n",
    "  - Accuracy\n",
    "  - Computational complexity\n",
    "  - Data needed\n",
    "  - Generalisation / ease of use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
