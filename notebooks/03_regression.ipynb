{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression intro\n",
    "\n",
    "- What is regression\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "- Linear model\n",
    "- Common approaches\n",
    "  - Least squares\n",
    "  - Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### California housing dataset\n",
    "\n",
    "The California housing dataset includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
      "       'Latitude', 'Longitude', 'MedHouseVal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "housing_data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "df = housing_data['frame']\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The housing data comprises of 8 features:\n",
    "\n",
    "- `MedInc`: Median income of the population in the block.\n",
    "- `HouseAge`: Average age of houses in the block.\n",
    "- `AveRooms`: Average number of rooms in a house in the block.\n",
    "- `AveBedrms`: Average number of bedrooms in a house in the block.\n",
    "- `Population`: Population of the block.\n",
    "- `AveOccup`: The average number of people living in a house.\n",
    "- `Latitude`: Latitudinal position.\n",
    "- `Logitude`: Longitudinal position.\n",
    "- `MedHouseVal`: The median value of houses in the block.\n",
    " \n",
    "The target feature is `MedHouseVal`.\n",
    "\n",
    "We shall now look at the datatypes for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the features are numerical, as is the target feature which makes regression easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical data\n",
    "\n",
    "As the data is geographical it is a good idea to visualise it as such.\n",
    "\n",
    "Using the `Latitude` and `Longitude` features we can plot the data as a scatter plot. Using the `alpha` parameter we can get an idea of population density across California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = df['Latitude']\n",
    "longitude = df['Longitude']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.title(\"Geographical Plot of California Housing Data\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.scatter(latitude, longitude, s=10, alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume the whitespace in the lower left corner is due to no houses being located in the ocean. This means the high density points are along the coast, which may have a high correlation with house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_val = df['MedHouseVal']\n",
    "population = df['Population']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.title(\"California Housing Value and Geographical Location\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.scatter(latitude, longitude, s=population/100, alpha=0.5, c=house_val, cmap='coolwarm')\n",
    "plt.colorbar().set_label(\"Median House Value (scale of 0-5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot above we can see the more expensive (red) areas are closely packed, with the cheaper (blue) areas being far more scattered, as well as (generally) being further away from the coast. We can assume the more expensive areas relate to Los Angeles (north) and San Fransisco (south). We will need to ensure geographical data is encoded into the model as location evidetely has an impact on house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and distribution\n",
    "\n",
    "To get a quick look at the characteristics of the dataset we can use the `describe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that some of the features have some large outlying values such as `AveRooms` and `AveOccup`, due to the max value being so much higher than the 75th percentile. We will have to handle this if we want to look at the histogram of the data as it may skew the results and not let us see the real distribution.\n",
    "\n",
    "As we have already seen geographical data has a large impact on house value we will not explore its correlation with the other features any further. We will find the correlation between each of the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_geo = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',\n",
    "           'Population', 'AveOccup', 'MedHouseVal']\n",
    "\n",
    "corr = df[non_geo].corr()\n",
    "print(corr['MedHouseVal'].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some inital results, such as income being highly correlated with house value but it will be easier to visualise as a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_title(\"Correlation of Non-Geographical Features\", fontsize=16)\n",
    "mat = ax.matshow(corr)\n",
    "ax.set_xticks(range(len(non_geo)))\n",
    "ax.set_yticks(range(len(non_geo)))\n",
    "ax.set_xticklabels(non_geo, rotation=-90, fontsize=12)\n",
    "ax.set_yticklabels(non_geo, fontsize=12)\n",
    "plt.colorbar(mat, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the highest values from some of the features with a long tail so we can get a better look at the disribution.\n",
    "\n",
    "There are also some anomalies with the median house value and house age features. It seems that they have cut-offs of 5 and 50 repsectively and so there are a high number of them in the final bin on the histogram. This will have to be dealt with as to not skew the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nohigh = df.astype(float)\n",
    "df_nohigh = df_nohigh.drop(df_nohigh[(df_nohigh['MedInc'] > 10) |\n",
    "                                     (df_nohigh['HouseAge'] > 50) |\n",
    "                                     (df_nohigh['AveRooms'] > 10) |\n",
    "                                     (df_nohigh['AveBedrms'] > 2) |\n",
    "                                     (df_nohigh['Population'] > 6000) |\n",
    "                                     (df_nohigh['AveOccup'] > 8) |\n",
    "                                     (df_nohigh['MedHouseVal'] > 4.7)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nohigh[non_geo].hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears most features have a lognormal distribution - that is they rise sharply with a long right-hand tail. This is due to a a small number of blocks having abnormally large, high value homes.\n",
    "\n",
    "Taking the log of these features should mean they are better fit by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nohighnormal = df_nohigh.copy()\n",
    "df_nohighnormal[\"MedInc\"] = np.log1p(df_nohighnormal['MedInc'])\n",
    "df_nohighnormal[\"AveRooms\"] = np.log1p(df_nohighnormal['AveRooms'])\n",
    "df_nohighnormal[\"AveBedrms\"] = np.log1p(df_nohighnormal['AveBedrms'])\n",
    "df_nohighnormal[\"Population\"] = np.log1p(df_nohighnormal['Population'])\n",
    "df_nohighnormal[\"AveOccup\"] = np.log1p(df_nohighnormal['AveOccup'])\n",
    "df_nohighnormal[\"MedHouseVal\"] = np.log1p(df_nohighnormal['MedHouseVal'])\n",
    "\n",
    "df_nohighnormal[non_geo].hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall apply this transformation to our dataframe ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = df.copy()\n",
    "df_normal[\"MedInc\"] = np.log1p(df_normal['MedInc'])\n",
    "df_normal[\"AveRooms\"] = np.log1p(df_normal['AveRooms'])\n",
    "df_normal[\"AveBedrms\"] = np.log1p(df_normal['AveBedrms'])\n",
    "df_normal[\"Population\"] = np.log1p(df_normal['Population'])\n",
    "df_normal[\"AveOccup\"] = np.log1p(df_normal['AveOccup'])\n",
    "df_normal[\"MedHouseVal\"] = np.log1p(df_normal['MedHouseVal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "Missing data will cause issues when it comes to training a model, so we will need to check for and deal with any missing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a very clean dataset and does not have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "As with missing values, any outliers in the data could also cause our model to underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = df_normal.copy()\n",
    "df_cut[non_geo].hist(bins=50, figsize=(16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the features `MedHouseVal` and `HouseAge` have been capped, this means there aren't any outliers but a large number of maximum values.\n",
    "\n",
    "Had these values not been capped the data would have followed more of a lognormal distribution. That is the right-hand tail would have stretched out further slowly decreasing.\n",
    "\n",
    "There is no way of recovering the lost data so instead we call cut off the cut-off. Later we shall scale the data so it should flatten out and be more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nohigh = df[non_geo].astype(float)\n",
    "df_cut = df_cut.drop(df_cut[(df_cut['HouseAge'] > 50) |\n",
    "                            (df_cut['MedHouseVal'] > 1.75)].index)\n",
    "\n",
    "df_cut.hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "It can be benificial to create additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cut.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df = df_cut.copy()\n",
    "\n",
    "extra_df['AveBedrmsPerRoom'] = extra_df['AveBedrms'] / extra_df['AveRooms']\n",
    "extra_df['AveBedrmsPerOccup'] = extra_df['AveBedrms'] / extra_df['AveOccup']\n",
    "extra_df['AveAddRooms'] = extra_df['AveRooms'] - extra_df['AveBedrms']\n",
    "extra_df['EstHouses'] = extra_df['Population'] / extra_df['AveOccup']\n",
    "\n",
    "new_features = ['MedHouseVal', 'AveBedrmsPerRoom', 'AveBedrmsPerOccup',\n",
    "                'AveAddRooms', 'EstHouses']\n",
    "\n",
    "new_features_corr = extra_df.corr()\n",
    "new_features_corr['MedHouseVal'].abs().sort_values(ascending=False)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# ax.set_title(\"Correlation of Non-Geographical Features\", fontsize=16)\n",
    "# mat = ax.matshow(new_features_corr)\n",
    "# ax.set_xticks(range(len(new_features)))\n",
    "# ax.set_yticks(range(len(new_features)))\n",
    "# ax.set_xticklabels(new_features, rotation=-90, fontsize=12)\n",
    "# ax.set_yticklabels(new_features, fontsize=12)\n",
    "# plt.colorbar(mat, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the new bedrooms per room and additional rooms features are both better features than the individual bedroom and room features. We shall keep these and remove the original features. Estimated houses also seems to have added some good information so we shall keep it in the feature set.\n",
    "\n",
    "The average bedrooms per person has not added any more information to the dataset so we shall remove this.\n",
    "\n",
    "As for the geographical data we shall encode this later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating geographical features\n",
    "\n",
    "As mentioned above, the geographical features `Latitude` and `Longitude` can be transformed into a single feature to represent distance to the closest city (LA and San Fransisco). The longitude and latitude seem to line up with real life positions so we shall get the positions of LA and San Fransisco from Google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA location as north-westerly peak\n",
    "la_lat, la_lon = 34.0522, -118.2437\n",
    "# SF location as south-easterly peak\n",
    "sf_lat, sf_lon = 37.7749, -122.4194\n",
    "\n",
    "extra_df['DistToLA'] = np.sqrt((extra_df['Latitude'] - la_lat)**2 +\n",
    "                               (extra_df['Longitude'] - la_lon)**2)\n",
    "extra_df['DistToSF'] = np.sqrt((extra_df['Latitude'] - sf_lat)**2 +\n",
    "                               (extra_df['Longitude'] - sf_lon)**2)\n",
    "\n",
    "extra_df['DistToCity'] = extra_df[['DistToLA', 'DistToSF']].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the correlation of this new feature with median house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_corr = extra_df.corr()\n",
    "city_corr['MedHouseVal'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created another great feature. As the distance to city feature is so much stronger than any other geographical feature we shall use this and drop the others.\n",
    "\n",
    "We shall also check the distribution of this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df['DistToCity'].hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution doesnt seem clear which is likely down to the very centre of the city not being the most expensive and decreasing as they go out.\n",
    "\n",
    "To add some more accuracy we can add some more known expensive areas around California. A quick google search lists the following cities as the top 10 most expensive areas as:\n",
    "\n",
    "- Atherton (37.4613° N 122.1997° W)\n",
    "- Santa Monica (34.0195° N, 118.4912° W)\n",
    "- Beverly Hills (34.0736° N, 118.4004° W)\n",
    "- Palo Alto (37.4419° N, 122.1430° W)\n",
    "- Los Altos (94022) (37.3852° N, 122.1141°W)\n",
    "- Ross (37.9624° N, 122.5550° W)\n",
    "- Portola Valley (37.3841° N, 122.2352° W)\n",
    "- Los Altos (94024) (37.3478° N, 122.1008° W)\n",
    "- Newport Beach (92661) (33.6189° N, 117.9298° W)\n",
    "- Newport Beach (92662) (33.6061° N, 117.8912° W)\n",
    "\n",
    "I shall also add a point for San Diego at 32.7157° N, 117.1611° W.\n",
    "\n",
    "We shall calculate the distances for each bloc to these areas and add a new feature to represent the miniumum of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_coords = np.array([[37.4613, -122.1997],\n",
    "                        [34.0195, -118.4912],\n",
    "                        [34.0736, -118.4004],\n",
    "                        [37.4419, -122.1430],\n",
    "                        [37.3852, -122.1141],\n",
    "                        [37.9624, -122.5550],\n",
    "                        [37.3841, -122.2352],\n",
    "                        [37.3478, -122.1008],\n",
    "                        [33.6189, -117.9298],\n",
    "                        [33.6061, -117.8912],\n",
    "                        [32.7157, -117.1611]])\n",
    "\n",
    "coords = np.asarray(extra_df[['Latitude', 'Longitude']])\n",
    "\n",
    "town_dists = np.asarray([np.sqrt((coords[:, 0] - town_coord[0])**2 + \n",
    "                                 (coords[:, 1] - town_coord[1])**2) \n",
    "                         for town_coord in town_coords])\n",
    "\n",
    "dist_to_town = np.min(town_dists, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the new feature and compare it to the original distance to city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df['DistToTown'] = dist_to_town\n",
    "extra_df['DistToTown'].hist(bins=50, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_corr = extra_df.corr()\n",
    "city_corr['MedHouseVal'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I hoped this had improved the correlation further.\n",
    "\n",
    "We can now transform this to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "normal_dist_to_town = qt.fit_transform(dist_to_town.reshape(-1, 1))\n",
    "\n",
    "extra_df['DistToTown'] = normal_dist_to_town\n",
    "extra_df['DistToTown'].hist(bins=100, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features\n",
    "\n",
    "Categorical features in the data usually need to be encoded into numerical features for a machine learning algorithm to learn from them, especially with a regression model.\n",
    "\n",
    "With this dataset there are only numerical features and all are of float type, so we can leave them as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "Now we have created all the features we shall used we shall apply scaling so features are centered with 0 mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(extra_df),\n",
    "                         columns=extra_df.columns)\n",
    "\n",
    "df_scaled.hist(figsize=(16, 16), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another scaling technique we can try is sklearns quantile transformer. It attempts to fit the data to a normal distribution which should help us when it comes to picking our Bayesian prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "normal_transformed_df = pd.DataFrame(qt.fit_transform(extra_df),\n",
    "                                     columns=extra_df.columns)\n",
    "\n",
    "normal_transformed_df.hist(figsize=(16, 16), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has worked well and we can clearly see that the new transformation has normally distributed most of the features.\n",
    "\n",
    "One notable outlier is the house age feature. My hypothesis is that house age cannot be normally distributed as both old and new houses can be expensive. It does not seem to be a feature that correlates well with house price so we may drop it later on when it comes to selecting the final features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "As discussed in part one reducing the number of dimensions can also be benificial for both the training time and generalisation of the model. This can be done by removing unnescessary features, but also by combining similar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = normal_transformed_df.corr()\n",
    "print(corr[\"MedHouseVal\"].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(normal_transformed_df[non_geo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the bedroom and room features as the new bedrooms per room feature has better correlation.\n",
    "\n",
    "As discussed earlier we shall remove all of the geographical features apart from `DistToTown`.\n",
    "\n",
    "We shall also remove the bedrooms per person feature we created as it does not provide any more useful information.\n",
    "\n",
    "The population and house age features also seem to have little correlation to house price also so we shall also remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_transformed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = ['AveRooms', 'AveBedrms', 'Latitude', 'Longitude',\n",
    "                      'DistToLA', 'DistToSF', 'DistToCity',\n",
    "                      'AveBedrmsPerOccup', 'Population', 'HouseAge']\n",
    "\n",
    "data = normal_transformed_df.drop(features_to_remove, axis=1, inplace=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()['MedHouseVal'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the most correlating features, whilst not duplicating any information between them.\n",
    "\n",
    "We have also reduced the dimensionality of the data which should aid both model training ease and generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on sampling\n",
    "\n",
    "- In group bias\n",
    "- Ground truth\n",
    "- Prior belief\n",
    "- Sampling bias\n",
    "- Selection bias\n",
    "- Bias variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Theorem\n",
    "\n",
    "- Bayes theorem\n",
    "- Bayes rule\n",
    "- Bayesian statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC\n",
    "\n",
    "- Markov chain\n",
    "- Markov property\n",
    "- MCMC\n",
    "- Metropolis Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian linear regression\n",
    "\n",
    "- What is bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior beliefs\n",
    "\n",
    "- Prior for Bayesian\n",
    "- Log-normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for model\n",
    "\n",
    "- Test train val split\n",
    "- Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spilt features and labels\n",
    "\n",
    "Seperate features and labels and create new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ['MedInc', 'AveOccup', 'AveBedrmsPerRoom',\n",
    "              'AveAddRooms', 'EstHouses', 'DistToTown']\n",
    "y_features = ['MedHouseVal']\n",
    "\n",
    "X = data[X_features]\n",
    "y = data[y_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get test and train sets\n",
    "\n",
    "Create testing and training sets. Combine features and labels into train and test data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat((y_train, X_train), axis=1)\n",
    "data_test = pd.concat((y_test, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing pipeline\n",
    "\n",
    "For convenience I have created a module which provides the above data pipeline.\n",
    "\n",
    "It will return the california housing data as 4 Pandas DataFrames:\n",
    "\n",
    "- X_train - training data\n",
    "- X_test - testing data\n",
    "- y_train - real house prices for training\n",
    "- y_test - real house prices for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import california_data_pipeline as cp\n",
    "import importlib\n",
    "importlib.reload(cp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = cp.load_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>AveBedrmsPerRoom</th>\n",
       "      <th>AveAddRooms</th>\n",
       "      <th>EstHouses</th>\n",
       "      <th>DistToTown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14735.000000</td>\n",
       "      <td>14735.000000</td>\n",
       "      <td>14735.000000</td>\n",
       "      <td>14735.000000</td>\n",
       "      <td>14735.000000</td>\n",
       "      <td>14735.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.001734</td>\n",
       "      <td>-0.003692</td>\n",
       "      <td>0.004260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.004026</td>\n",
       "      <td>1.001917</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>0.999165</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.997231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.201766</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.676394</td>\n",
       "      <td>-0.670385</td>\n",
       "      <td>-0.674644</td>\n",
       "      <td>-0.677818</td>\n",
       "      <td>-0.684667</td>\n",
       "      <td>-0.666649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>-0.004090</td>\n",
       "      <td>-0.001048</td>\n",
       "      <td>-0.006362</td>\n",
       "      <td>0.002993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.673733</td>\n",
       "      <td>0.686719</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.672609</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.672975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      AveOccup  AveBedrmsPerRoom   AveAddRooms  \\\n",
       "count  14735.000000  14735.000000      14735.000000  14735.000000   \n",
       "mean      -0.002548      0.006250         -0.000037     -0.001734   \n",
       "std        1.004026      1.001917          0.999662      0.999165   \n",
       "min       -5.199338     -3.201766         -5.199338     -5.199338   \n",
       "25%       -0.676394     -0.670385         -0.674644     -0.677818   \n",
       "50%        0.003764      0.002503         -0.004090     -0.001048   \n",
       "75%        0.673733      0.686719          0.676533      0.672609   \n",
       "max        5.199338      5.199338          5.199338      5.199338   \n",
       "\n",
       "          EstHouses    DistToTown  \n",
       "count  14735.000000  14735.000000  \n",
       "mean      -0.003692      0.004260  \n",
       "std        1.002058      0.997231  \n",
       "min       -5.199338     -5.199338  \n",
       "25%       -0.684667     -0.666649  \n",
       "50%       -0.006362      0.002993  \n",
       "75%        0.678650      0.672975  \n",
       "max        5.199338      5.199338  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14735.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.003381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.199338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.670557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.001255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.667873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.199338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MedHouseVal\n",
       "count  14735.000000\n",
       "mean      -0.003381\n",
       "std        1.000829\n",
       "min       -5.199338\n",
       "25%       -0.670557\n",
       "50%       -0.001255\n",
       "75%        0.667873\n",
       "max        5.199338"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance measure\n",
    "\n",
    "- Define the performance measure of the model.\n",
    "- MSE / RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "\n",
    "Create initial Bayesian linear regression model with guassian priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normal_linreg(data, formula):\n",
    "#     with pm.Model() as model:\n",
    "#         # Normal prior\n",
    "#         family = pm.glm.families.Normal()\n",
    "#         # Create model from formula\n",
    "#         pm.GLM.from_formula(formula, data=data, family=family)\n",
    "#         return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model sampler\n",
    "\n",
    "Method to sample model and return trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_model(n_samples=1000, n_tune=500, n_cores=2):\n",
    "#     trace = pm.sample(draws=n_samples, tune=n_tune,\n",
    "#                       progressbar=True, cores=n_cores)\n",
    "#     return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula = \"MedHouseVal ~ MedInc + AveOccup + AveBedrmsPerRoom +\\\n",
    "#            AveAddRooms + EstHouses + DistToCity\"\n",
    "\n",
    "# with normal_linreg(data_train, formula):\n",
    "#     normal_linreg_trace = sample_model(n_samples=5000, n_tune=500,\n",
    "#                                        n_cores=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.save_trace(normal_linreg_trace, directory=\"traces\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of model\n",
    "\n",
    "- Evaluate model performance on val / test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse the sample trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.traceplot(normal_linreg_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.plot_posterior(normal_linreg_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.forestplot(normal_linreg_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate mae and rmse for model on X, y\n",
    "# def score(trace, X, y):\n",
    "#     # get mean of coefficients\n",
    "#     all_coeffs = np.asarray([trace[name] for name in trace.varnames])\n",
    "#     coeff_means = all_coeffs.mean(axis=1)\n",
    "#     # set intercept and variable coefficients\n",
    "#     n_coeffs = X.shape[1]\n",
    "#     coeffs = coeff_means[:n_coeffs+1]\n",
    "#     # get predictions\n",
    "#     X = np.column_stack((np.ones((X.shape[0])), X))\n",
    "#     preds = np.dot(coeffs, X.T)\n",
    "#     # calculate mae, rmse\n",
    "#     labels = np.asarray(y).reshape(-1)\n",
    "#     errors = preds - labels\n",
    "#     mae = np.mean(abs(errors))\n",
    "#     rmse = np.sqrt(np.mean(errors**2))\n",
    "#     return (mae, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae, rmse = score(normal_linreg_trace, X_test, y_test)\n",
    "\n",
    "# print(\"Bayesian Linear Regression Mean Results\")\n",
    "# print(f\"Mean absolute error     (MAE)  : {mae:.3f}\")\n",
    "# print(f\"Root mean squared error (RMSE) : {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to standard linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression R2: 0.6244136481772364\n",
      "Linear regression MSE: 0.3728552495507357\n",
      "Linear regression RMSE 0.6106187432029381\n"
     ]
    }
   ],
   "source": [
    "linreg_preds = linreg.predict(X_test)\n",
    "linreg_r2 = r2_score(y_test, linreg_preds)\n",
    "linreg_mse = mean_squared_error(y_test, linreg_preds)\n",
    "linreg_rmse = mean_squared_error(y_test, linreg_preds, squared=False)\n",
    "print(\"Linear regression R2:\", linreg_r2)\n",
    "print(\"Linear regression MSE:\", linreg_mse)\n",
    "print(\"Linear regression RMSE\", linreg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn wrapper\n",
    "\n",
    "I have created a scikit-learn wrapper class for the model above which gives the model sklearn functionality.\n",
    "\n",
    "Create an instance of `BayesianLinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesian_linear_regressor as br\n",
    "import importlib\n",
    "importlib.reload(br)\n",
    "\n",
    "bayesianreg = br.BayesianLinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `fit()` with the training features `X` and real values `y` with the model formula create and sample from the model.\n",
    "\n",
    "Additional parameters `n_samples`, `n_tune` and `n_cores` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling with formula:\n",
      "MedHouseVal ~ MedInc + AveOccup + AveBedrmsPerRoom + AveAddRooms + EstHouses + DistToTown\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (8 chains in 8 jobs)\n",
      "NUTS: [sd, DistToTown, EstHouses, AveAddRooms, AveBedrmsPerRoom, AveOccup, MedInc, Intercept]\n",
      "Sampling 8 chains, 0 divergences: 100%|██████████| 56000/56000 [00:44<00:00, 1255.00draws/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianLinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesianreg.fit(X_train, y_train, n_samples=5000, n_tune=2000, n_cores=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also instanciate the objects model and trace attributes:`bayesianreg.model_` and RMSE: `bayesianreg.trace_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `predict` with test features `X` to return predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bayesianreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also instanciate the objects mean coefficients attribute:`bayesianreg.mean_coeffs_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00173039  0.62151252 -0.14779621  0.09071183  0.02570703  0.0339327\n",
      " -0.3963465  -0.50286574  0.6048055 ]\n"
     ]
    }
   ],
   "source": [
    "print(bayesianreg.mean_coeffs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `score()` with test features `X` and real values `y` to return the R2 score of the model on the training data.\n",
    "\n",
    "This will also instanciate the objects attributes for MAE:`bayesianreg.mae_` and RMSE: `bayesianreg.rmse_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian linear regression R2: 0.6244127595825468\n",
      "Bayesian linear regression MSE:  0.3728561316839473\n",
      "Bayesian linear regression RMSE: 0.6106194655298399\n"
     ]
    }
   ],
   "source": [
    "r2 = bayesianreg.score(X_test, y_test)\n",
    "print(f\"Bayesian linear regression R2: {r2}\")\n",
    "print(f\"Bayesian linear regression MSE:  {bayesianreg.mse_}\")\n",
    "print(f\"Bayesian linear regression RMSE: {bayesianreg.rmse_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding this functionality will allow use to use ensemble methods with other scikit-learn models using the `StackingRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trace_dict = dict()\n",
    "for nm in ['k1', 'k2', 'k3', 'k4', 'k5']:\n",
    "    models_lin[nm].name = 'poly=lin, '+nm\n",
    "    model_trace_dict.update({models_lin[nm]: traces_lin[nm]})\n",
    "\n",
    "    models_quad[nm].name = 'poly=quad, '+nm\n",
    "    model_trace_dict.update({models_quad[nm]: traces_quad[nm]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwaic = pm.compare(model_trace_dict, ic='WAIC')\n",
    "dfwaic.index = pd.MultiIndex.from_tuples(\n",
    "    [tuple(k.split(',')) for k,v in dfwaic.iterrows()])\n",
    "\n",
    "dfwaic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian linear regression optimisation\n",
    "\n",
    "- Ridge / lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression conclusion\n",
    "\n",
    "- Concluding points on things learnt from dataset.\n",
    "- Concluding points of Bayesian regression performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
