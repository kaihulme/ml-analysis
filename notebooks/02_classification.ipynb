{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "- What is supervised learning?\n",
    "- Training\n",
    "- Fitting\n",
    "- Labels\n",
    "- Error / cost / objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "- What is classification?\n",
    "- Different approaches\n",
    "  - Logistic regression - log loss\n",
    "  - ANNs, SVMs\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No free lunch\n",
    "\n",
    "- Model selection.\n",
    "- Parametric / nonparametric models.\n",
    "- Discriminative / generative modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with MNIST\n",
    "\n",
    "- The aim of classification with MNIST\n",
    "- Get the data\n",
    "- Recap analysis\n",
    "  - Size, shape, type\n",
    "  - Features\n",
    "  - Missing data, outliers etc.\n",
    "  - Target feature of dataset (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall look at the number of unique features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 unique digit types.\n",
    "\n",
    "We shall put the data into a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784), y shape: (70000,)\n",
      "Image shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "Now we shall have a look at the image data to see what form it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X[0]), np.max(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the minimum and maximum values in appears that pixel are of the standard 0-255 intensity form. \n",
    "\n",
    "It is beneficial to some machine learning methods for values to be normalised between 0-1, notably neural networks so we shall do this in the data preparation step.\n",
    "\n",
    "Finally we shall check the datatypes for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are of float type as expected but it may be beneficial to convert the string type labels to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "  - train, val test sets\n",
    "  - cross validation\n",
    "  - reshape / retype\n",
    "\n",
    "As stated earlier normalising the feature values will be beneficial for machine learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_full = X / 255.0\n",
    "print(np.min(X_full), np.max(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also now convert the string type features to integers for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "y_full = y.astype('int64')\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create a train, validation and test set before we proceed using sklearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "44800\n",
      "11200\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print(X_full.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "- Neurons structure\n",
    "- Action potentials ~ activation functions\n",
    "- Weights, connections etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "\n",
    "- Perceptron history\n",
    "- Input layer\n",
    "- Output layer\n",
    "- Activation function\n",
    "- Weights\n",
    "- Bias\n",
    "- Try and apply it to MNIST (lots of neurons?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8815178571428571"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron regularisation\n",
    "\n",
    "This is a reasonably good result for such a simple model, but this is due to the simplicity of the data set.\n",
    "\n",
    "One way we can try and improve the results is through regularisation.\n",
    "\n",
    "** **EXPLAIN REGULARISATION** **\n",
    "\n",
    "As the model training doesn't take too long we shall try a grid search with `None`, `l1`, `l2` and `ElastiNet` regularisation.\n",
    "\n",
    "We shall do this with 5-fold cross validation, evaluating the accuracy on the validation set.\n",
    "\n",
    "** **EXPLAIN CROSS VALIDATION** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.866, test=0.848), total=   2.2s\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.883, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.876, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n",
      "[CV] .... penalty=None, score=(train=0.885, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n",
      "[CV] .... penalty=None, score=(train=0.853, test=0.841), total=   2.2s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.877, test=0.860), total=   2.2s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.885, test=0.877), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.872, test=0.865), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.885, test=0.875), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.863, test=0.853), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.851, test=0.841), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.851, test=0.849), total=   2.2s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.817, test=0.816), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.853, test=0.847), total=   2.2s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.801, test=0.798), total=   2.2s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.851, test=0.841), total=   2.4s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.851, test=0.849), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.817, test=0.816), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.853, test=0.847), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.801, test=0.798), total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Perceptron(early_stopping=True, n_jobs=-1,\n",
       "                                  random_state=42),\n",
       "             param_grid=[{'penalty': ['None', 'l1', 'l2', 'elasticnet']}],\n",
       "             return_train_score=True, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_perceptron = Perceptron(alpha=0.0001, tol=0.001,\n",
    "                            max_iter=1000, early_stopping=True,\n",
    "                            validation_fraction=0.1, n_iter_no_change=5,\n",
    "                            random_state=42, verbose=0, n_jobs=-1)\n",
    "\n",
    "perceptron_params = [{'penalty': ['None', 'l1', 'l2', 'elasticnet']}]\n",
    "\n",
    "grid_search = GridSearchCV(reg_perceptron, perceptron_params,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           return_train_score=True,\n",
    "                           verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the optimal parameters of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l1'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears l1 regularisation has produced the best results for the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "- Multiple layers of neurons\n",
    "- Able to learn more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPs\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Epochs, convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "- Calculation of outputs from input\n",
    "- Calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "- Gradient descent\n",
    "- Learning rate\n",
    "- Minimise loss function\n",
    "- Convex optimisation problem\n",
    "- Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- What is SGD\n",
    "- Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "- What is cross entropy\n",
    "- Sparse categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST\n",
    "\n",
    "- Feed forward network\n",
    "- Dense layers\n",
    "- Epochs, convergence\n",
    "- Create a simple MLP for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 9s 190us/sample - loss: 0.7054 - accuracy: 0.8126 - val_loss: 0.3659 - val_accuracy: 0.8951\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 7s 167us/sample - loss: 0.3244 - accuracy: 0.9065 - val_loss: 0.2966 - val_accuracy: 0.9142\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 183us/sample - loss: 0.2734 - accuracy: 0.9206 - val_loss: 0.2621 - val_accuracy: 0.9229\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.2413 - accuracy: 0.9305 - val_loss: 0.2313 - val_accuracy: 0.9306\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 9s 193us/sample - loss: 0.2161 - accuracy: 0.9379 - val_loss: 0.2104 - val_accuracy: 0.9385\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 168us/sample - loss: 0.1953 - accuracy: 0.9432 - val_loss: 0.1974 - val_accuracy: 0.9411\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 9s 206us/sample - loss: 0.1783 - accuracy: 0.9484 - val_loss: 0.1841 - val_accuracy: 0.9458\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.1633 - accuracy: 0.9522 - val_loss: 0.1700 - val_accuracy: 0.9489\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 9s 193us/sample - loss: 0.1501 - accuracy: 0.9563 - val_loss: 0.1669 - val_accuracy: 0.9489\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 9s 191us/sample - loss: 0.1388 - accuracy: 0.9601 - val_loss: 0.1513 - val_accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing model training\n",
    "\n",
    "We shall use Tensorboard to visualise our model results as well as analyse training.\n",
    "\n",
    "We shall configure it to create a new subdirectory for each model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tb_dir():\n",
    "    curr_dir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "    tb_dir = time.strftime(\"model_%Y_%m_%d-%H-%M-%S\")\n",
    "    return os.path.join(curr_dir, tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a callback during model training for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(get_tb_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with the callback will then write the logs to it's own directory in `tensorboard_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.7610 - accuracy: 0.8044 - val_loss: 0.3651 - val_accuracy: 0.8956\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 8s 185us/sample - loss: 0.3232 - accuracy: 0.9075 - val_loss: 0.2893 - val_accuracy: 0.9163\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 180us/sample - loss: 0.2702 - accuracy: 0.9229 - val_loss: 0.2627 - val_accuracy: 0.9226\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 170us/sample - loss: 0.2387 - accuracy: 0.9313 - val_loss: 0.2281 - val_accuracy: 0.9337\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 8s 174us/sample - loss: 0.2151 - accuracy: 0.9384 - val_loss: 0.2086 - val_accuracy: 0.9376\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 183us/sample - loss: 0.1963 - accuracy: 0.9435 - val_loss: 0.1923 - val_accuracy: 0.9430\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 9s 199us/sample - loss: 0.1788 - accuracy: 0.9485 - val_loss: 0.1820 - val_accuracy: 0.9454\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 9s 204us/sample - loss: 0.1644 - accuracy: 0.9526 - val_loss: 0.1653 - val_accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 9s 206us/sample - loss: 0.1517 - accuracy: 0.9566 - val_loss: 0.1592 - val_accuracy: 0.9533\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 10s 216us/sample - loss: 0.1404 - accuracy: 0.9595 - val_loss: 0.1531 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill 132185\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=./tensorboard_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper neural networks\n",
    "\n",
    "- Create a large overfitting network\n",
    "- Early stopping, check pointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 382,510\n",
      "Trainable params: 382,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "for i in range(30):\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 31s 702us/sample - loss: 2.3010 - accuracy: 0.1107 - val_loss: 2.2977 - val_accuracy: 0.1117\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 25s 560us/sample - loss: 2.1895 - accuracy: 0.1740 - val_loss: 1.9324 - val_accuracy: 0.2107\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 24s 546us/sample - loss: 1.7827 - accuracy: 0.2547 - val_loss: 1.4934 - val_accuracy: 0.3879\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 26s 573us/sample - loss: 1.3255 - accuracy: 0.4532 - val_loss: 1.1656 - val_accuracy: 0.5719\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 25s 565us/sample - loss: 1.0921 - accuracy: 0.5877 - val_loss: 1.0476 - val_accuracy: 0.5654\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 25s 556us/sample - loss: 1.0685 - accuracy: 0.6449 - val_loss: 2.2953 - val_accuracy: 0.1204\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 24s 540us/sample - loss: 1.6754 - accuracy: 0.3902 - val_loss: 1.4036 - val_accuracy: 0.5146\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 25s 548us/sample - loss: 1.3509 - accuracy: 0.5040 - val_loss: 1.3469 - val_accuracy: 0.4722\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 25s 560us/sample - loss: 1.1062 - accuracy: 0.5958 - val_loss: 1.1570 - val_accuracy: 0.5535\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 23s 522us/sample - loss: 0.8357 - accuracy: 0.7169 - val_loss: 0.7916 - val_accuracy: 0.7399\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 24s 525us/sample - loss: 0.7153 - accuracy: 0.7799 - val_loss: 1.2555 - val_accuracy: 0.6251\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 24s 547us/sample - loss: 1.3032 - accuracy: 0.5212 - val_loss: 1.0179 - val_accuracy: 0.6300\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 32s 711us/sample - loss: 1.2577 - accuracy: 0.5391 - val_loss: 1.1494 - val_accuracy: 0.5826\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 26s 591us/sample - loss: 1.2483 - accuracy: 0.5655 - val_loss: 0.9019 - val_accuracy: 0.7054\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 29s 637us/sample - loss: 0.8633 - accuracy: 0.6983 - val_loss: 0.7694 - val_accuracy: 0.7221\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 29s 656us/sample - loss: 0.6927 - accuracy: 0.7891 - val_loss: 0.6174 - val_accuracy: 0.8371\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 33s 743us/sample - loss: 0.5895 - accuracy: 0.8394 - val_loss: 0.4367 - val_accuracy: 0.8938\n",
      "Epoch 18/20\n",
      "44800/44800 [==============================] - 33s 740us/sample - loss: 0.5190 - accuracy: 0.8578 - val_loss: 1.1536 - val_accuracy: 0.5560\n",
      "Epoch 19/20\n",
      "44800/44800 [==============================] - 31s 692us/sample - loss: 0.4885 - accuracy: 0.8666 - val_loss: 0.4141 - val_accuracy: 0.8950\n",
      "Epoch 20/20\n",
      "44800/44800 [==============================] - 30s 660us/sample - loss: 0.4883 - accuracy: 0.8665 - val_loss: 0.9130 - val_accuracy: 0.7412\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this large model with almost 400,000 parameters was a lot harder to train and generalised far worse on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / exploding gradient problem\n",
    "\n",
    "- What is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shall create a Keras wrapper in order to use Scikit's grid search.\n",
    "\n",
    "With this wrapper we can search for params: \n",
    "- `kernel_initializer`: weigh and bias initialisation method\n",
    "- `optimizer`: optimisation method used in back progogation\n",
    "- `activation`: activation function used in forward pass\n",
    "- `lr`: learning rate of the optimiser\n",
    "- `momentum`: momentum of the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns keras optimser with lr and momentum\n",
    "def get_opt(opt, lr, m):\n",
    "    if (opt == 'sgd'):\n",
    "        return SGD(lr=lr, momentum=m)\n",
    "    elif (opt == 'adam'):\n",
    "        return Adam(lr=lr, momentum=m)\n",
    "    return False\n",
    "\n",
    "\n",
    "# Creates model for KerasClassifier\n",
    "def create_model(init='glorot_uniform', opt='sgd', act='relu',\n",
    "                 lr=0.01, m=0):\n",
    "    # sequential model\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(Dense(100, input_shape=[784],\n",
    "                    kernel_initializer=init, activation=act))\n",
    "    # hidden layers\n",
    "    for i in range(10):\n",
    "        model.add(Dense(100, kernel_initializer=init,\n",
    "                        activation=act))\n",
    "    # output layer\n",
    "    model.add(Dense(10, kernel_initializer=init,\n",
    "                    activation=\"softmax\"))\n",
    "    # optimiser\n",
    "    opt = get_opt(opt, lr, m)\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "model = KerasClassifier(build_fn=create_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initlialisation\n",
    "\n",
    "- GloRoot\n",
    "- He\n",
    "- LeCeun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-saturating activation functions\n",
    "\n",
    "- Leaky RELU\n",
    "\n",
    "- SELU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "- Batch normalisation\n",
    "- Mini-batch\n",
    "- Implement batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "- Momentum\n",
    "    - For SGD\n",
    "- Better optimisers\n",
    "    - Adam\n",
    "    - Compare some others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "- Analyse adjusting learning rate\n",
    "- Learning rate schedling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in ANNs\n",
    "\n",
    "- Why large networks can overfit\n",
    "- l1, l2 loss and regularisation\n",
    "- Implement regularisation\n",
    "- Dropout, monte carlo dropout\n",
    "- Create a network with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks (CNN)\n",
    "\n",
    "- Problems with neural networks and images\n",
    "- Convolutions, filters\n",
    "- Convolutional layers, feature maps\n",
    "- Simple CNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training better CNNs\n",
    "\n",
    "- Stride, step\n",
    "- Pooling, max pooling\n",
    "- Better CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "- What is transfer learning\n",
    "- Use transfer learning with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of ANNs\n",
    "\n",
    "- Problems with ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines (SVMs)\n",
    "\n",
    "Train an SVM (with a chosen Kernel) and perform the same analyses as for ANNs. Interpret  and  discuss  your  results. Does the model overfit? How do they compare with ANNs? And why? How does the type of kernel (e.g.linear, RBF, etc.) impact on performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines\n",
    "\n",
    "- Hyperplane\n",
    "- Support vectors\n",
    "- Minimising lagrange multiplier (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "- Implementation of SVM on MNIST\n",
    "- Plot decision boundaries from lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic SVM\n",
    "\n",
    "- What is a probabilistic SVM?\n",
    "- SVM on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dual problem\n",
    "\n",
    "What is the dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "- Different representations\n",
    "- Kernels\n",
    "- Mercer's theorem\n",
    "- Gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel trick\n",
    "\n",
    "- What is the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVMs\n",
    "\n",
    "- How kernel SVMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial KSVM\n",
    "\n",
    "- What is the polynomial kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF) KSVM\n",
    "\n",
    "- What is the RBF kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid KSVM\n",
    "\n",
    "- What is the sigmoid kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising SVMs\n",
    "\n",
    "- Hinge loss\n",
    "- Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of SVMs\n",
    "\n",
    "- Problems with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of ANNs and SVMs\n",
    "\n",
    "- Comparison of neural networks and support vector machines.\n",
    "- Area under ROC curve\n",
    "- Precision, recall, accuracy...\n",
    "- TP, TN, FP, FN and rates for each\n",
    "- Confusion matrix\n",
    "- Metric\n",
    "- Validation\n",
    "- Cross-validation\n",
    "- Prediction\n",
    "- Inference\n",
    "- Interpretability\n",
    "- Sensitivity vs specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
