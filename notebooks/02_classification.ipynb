{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.activations import relu, selu\n",
    "from tensorflow.keras.layers import Activation, LeakyReLU, PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "- What is supervised learning?\n",
    "- Training\n",
    "- Fitting\n",
    "- Labels\n",
    "- Error / cost / objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "- What is classification?\n",
    "- Different approaches\n",
    "  - Logistic regression - log loss\n",
    "  - ANNs, SVMs\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No free lunch\n",
    "\n",
    "- Model selection.\n",
    "- Parametric / nonparametric models.\n",
    "- Discriminative / generative modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with MNIST\n",
    "\n",
    "- The aim of classification with MNIST\n",
    "- Get the data\n",
    "- Recap analysis\n",
    "  - Size, shape, type\n",
    "  - Features\n",
    "  - Missing data, outliers etc.\n",
    "  - Target feature of dataset (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall look at the number of unique features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 unique digit types.\n",
    "\n",
    "We shall put the data into a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784), y shape: (70000,)\n",
      "Image shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "Now we shall have a look at the image data to see what form it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X[0]), np.max(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the minimum and maximum values in appears that pixel are of the standard 0-255 intensity form. \n",
    "\n",
    "It is beneficial to some machine learning methods for values to be normalised between 0-1, notably neural networks so we shall do this in the data preparation step.\n",
    "\n",
    "Finally we shall check the datatypes for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are of float type as expected but it may be beneficial to convert the string type labels to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "  - train, val test sets\n",
    "  - cross validation\n",
    "  - reshape / retype\n",
    "\n",
    "As stated earlier normalising the feature values will be beneficial for machine learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_full = X / 255.0\n",
    "print(np.min(X_full), np.max(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also now convert the string type features to integers for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "y_full = y.astype('int64')\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create a train, validation and test set before we proceed using sklearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "44800\n",
      "11200\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print(X_full.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "- Neurons structure\n",
    "- Action potentials ~ activation functions\n",
    "- Weights, connections etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "\n",
    "- Perceptron history\n",
    "- Input layer\n",
    "- Output layer\n",
    "- Activation function\n",
    "- Weights\n",
    "- Bias\n",
    "- Try and apply it to MNIST (lots of neurons?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8815178571428571"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron regularisation\n",
    "\n",
    "This is a reasonably good result for such a simple model, but this is due to the simplicity of the data set.\n",
    "\n",
    "One way we can try and improve the results is through regularisation.\n",
    "\n",
    "** **EXPLAIN REGULARISATION** **\n",
    "\n",
    "As the model training doesn't take too long we shall try a grid search with `None`, `l1`, `l2` and `ElastiNet` regularisation.\n",
    "\n",
    "We shall do this with 5-fold cross validation, evaluating the accuracy on the validation set.\n",
    "\n",
    "** **EXPLAIN CROSS VALIDATION** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.866, test=0.848), total=   2.2s\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.883, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.876, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n",
      "[CV] .... penalty=None, score=(train=0.885, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n",
      "[CV] .... penalty=None, score=(train=0.853, test=0.841), total=   2.2s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.877, test=0.860), total=   2.2s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.885, test=0.877), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.872, test=0.865), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.885, test=0.875), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.863, test=0.853), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.851, test=0.841), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.851, test=0.849), total=   2.2s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.817, test=0.816), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.853, test=0.847), total=   2.2s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.801, test=0.798), total=   2.2s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.851, test=0.841), total=   2.4s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.851, test=0.849), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.817, test=0.816), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.853, test=0.847), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.801, test=0.798), total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Perceptron(early_stopping=True, n_jobs=-1,\n",
       "                                  random_state=42),\n",
       "             param_grid=[{'penalty': ['None', 'l1', 'l2', 'elasticnet']}],\n",
       "             return_train_score=True, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_perceptron = Perceptron(alpha=0.0001, tol=0.001,\n",
    "                            max_iter=1000, early_stopping=True,\n",
    "                            validation_fraction=0.1, n_iter_no_change=5,\n",
    "                            random_state=42, verbose=0, n_jobs=-1)\n",
    "\n",
    "perceptron_params = [{'penalty': ['None', 'l1', 'l2', 'elasticnet']}]\n",
    "\n",
    "grid_search = GridSearchCV(reg_perceptron, perceptron_params,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           return_train_score=True,\n",
    "                           verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the optimal parameters of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l1'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears l1 regularisation has produced the best results for the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "- Multiple layers of neurons\n",
    "- Able to learn more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPs\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Epochs, convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "- Calculation of outputs from input\n",
    "- Calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "- Gradient descent\n",
    "- Learning rate\n",
    "- Minimise loss function\n",
    "- Convex optimisation problem\n",
    "- Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- What is SGD\n",
    "- Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "- What is cross entropy\n",
    "- Sparse categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST\n",
    "\n",
    "- Feed forward network\n",
    "- Dense layers\n",
    "- Epochs, convergence\n",
    "- Create a simple MLP for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 9s 190us/sample - loss: 0.7054 - accuracy: 0.8126 - val_loss: 0.3659 - val_accuracy: 0.8951\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 7s 167us/sample - loss: 0.3244 - accuracy: 0.9065 - val_loss: 0.2966 - val_accuracy: 0.9142\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 183us/sample - loss: 0.2734 - accuracy: 0.9206 - val_loss: 0.2621 - val_accuracy: 0.9229\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.2413 - accuracy: 0.9305 - val_loss: 0.2313 - val_accuracy: 0.9306\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 9s 193us/sample - loss: 0.2161 - accuracy: 0.9379 - val_loss: 0.2104 - val_accuracy: 0.9385\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 168us/sample - loss: 0.1953 - accuracy: 0.9432 - val_loss: 0.1974 - val_accuracy: 0.9411\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 9s 206us/sample - loss: 0.1783 - accuracy: 0.9484 - val_loss: 0.1841 - val_accuracy: 0.9458\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.1633 - accuracy: 0.9522 - val_loss: 0.1700 - val_accuracy: 0.9489\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 9s 193us/sample - loss: 0.1501 - accuracy: 0.9563 - val_loss: 0.1669 - val_accuracy: 0.9489\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 9s 191us/sample - loss: 0.1388 - accuracy: 0.9601 - val_loss: 0.1513 - val_accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing model training\n",
    "\n",
    "We shall use Tensorboard to visualise our model results as well as analyse training.\n",
    "\n",
    "We shall configure it to create a new subdirectory for each model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tb_dir():\n",
    "    curr_dir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "    tb_dir = time.strftime(\"model_%Y_%m_%d-%H-%M-%S\")\n",
    "    return os.path.join(curr_dir, tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a callback during model training for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(get_tb_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with the callback will then write the logs to it's own directory in `tensorboard_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.7610 - accuracy: 0.8044 - val_loss: 0.3651 - val_accuracy: 0.8956\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 8s 185us/sample - loss: 0.3232 - accuracy: 0.9075 - val_loss: 0.2893 - val_accuracy: 0.9163\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 180us/sample - loss: 0.2702 - accuracy: 0.9229 - val_loss: 0.2627 - val_accuracy: 0.9226\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 170us/sample - loss: 0.2387 - accuracy: 0.9313 - val_loss: 0.2281 - val_accuracy: 0.9337\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 8s 174us/sample - loss: 0.2151 - accuracy: 0.9384 - val_loss: 0.2086 - val_accuracy: 0.9376\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 183us/sample - loss: 0.1963 - accuracy: 0.9435 - val_loss: 0.1923 - val_accuracy: 0.9430\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 9s 199us/sample - loss: 0.1788 - accuracy: 0.9485 - val_loss: 0.1820 - val_accuracy: 0.9454\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 9s 204us/sample - loss: 0.1644 - accuracy: 0.9526 - val_loss: 0.1653 - val_accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 9s 206us/sample - loss: 0.1517 - accuracy: 0.9566 - val_loss: 0.1592 - val_accuracy: 0.9533\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 10s 216us/sample - loss: 0.1404 - accuracy: 0.9595 - val_loss: 0.1531 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !kill 132185\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=./tensorboard_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper neural networks\n",
    "\n",
    "- Create a large overfitting network\n",
    "- Early stopping, check pointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 382,510\n",
      "Trainable params: 382,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "for i in range(30):\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this large model with almost 400,000 parameters was a lot harder to train and generalised far worse on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / exploding gradient problem\n",
    "\n",
    "- What is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shall create a Keras wrapper in order to use Scikit's grid search.\n",
    "\n",
    "With this wrapper we can search for params: \n",
    "- `kernel_initializer`: weigh and bias initialisation method\n",
    "- `optimizer`: optimisation method used in back progogation\n",
    "- `activation`: activation function used in forward pass\n",
    "- `lr`: learning rate of the optimiser\n",
    "- `momentum`: momentum of the optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initlialisation\n",
    "\n",
    "- GloRoot\n",
    "- He\n",
    "- LeCeun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_init_model(init):\n",
    "    # Sequential model\n",
    "    model = Sequential()\n",
    "    # Input layer with activation layer\n",
    "    model.add(Dense(100, kernel_input_shape=[784]))\n",
    "    model.add(get_activation(activation))\n",
    "    # Hidden layers with activation function\n",
    "    for i in range(21):\n",
    "        model.add(Dense(50))\n",
    "        model.add(get_activation(activation))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = KerasClassifier(build_fn=create_init_model, epochs=10)\n",
    "param_grid = {\"init\": ['lecun_uniform', 'glorot_normal', 'glorot_uniform',\n",
    "                       'he_normal', 'he_uniform']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-saturating activation functions\n",
    "\n",
    "- Leaky RELU\n",
    "- SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return activation function as layer\n",
    "def get_activation(activation):\n",
    "    if (activation == 'relu'):\n",
    "        return Activation(relu)\n",
    "    elif (activation == 'selu'):\n",
    "        return Activation(selu)\n",
    "    elif (activation == 'leakyrelu'):\n",
    "        return LeakyReLU()\n",
    "    elif (activation == 'prelu'):\n",
    "        return PReLU()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "    # Sequential model\n",
    "    model = Sequential()\n",
    "    # Input layer with activation layer\n",
    "    model.add(Dense(100, input_shape=[784]))\n",
    "    model.add(get_activation(activation))\n",
    "    # Hidden layers with activation function\n",
    "    for i in range(21):\n",
    "        model.add(Dense(50))\n",
    "        model.add(get_activation(activation))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "act_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"activation\": ['relu', 'selu', 'leakyrelu', 'prelu']}\n",
    "\n",
    "# grid search\n",
    "act_grid = RandomizedSearchCV(act_model, param_grid, n_iter=4, cv=2, verbose=0)\n",
    "act_grid_results = act_grid.fit(X_train, y_train,\n",
    "                                validation_data=[X_val, y_val],\n",
    "                                callbacks=[tensorboard, early_stopping],\n",
    "                                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'prelu'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_grid_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "- Batch normalisation\n",
    "- Mini-batch\n",
    "- Implement batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_21 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 139,846\n",
      "Trainable params: 136,178\n",
      "Non-trainable params: 3,668\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "for i in range(20):\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 46s 1ms/sample - loss: 1.4566 - accuracy: 0.5210 - val_loss: 0.8362 - val_accuracy: 0.7379\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 43s 956us/sample - loss: 0.8345 - accuracy: 0.7538 - val_loss: 0.4975 - val_accuracy: 0.8545\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 39s 861us/sample - loss: 0.6466 - accuracy: 0.8164 - val_loss: 0.4047 - val_accuracy: 0.8841\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 40s 886us/sample - loss: 0.5597 - accuracy: 0.8455 - val_loss: 0.3630 - val_accuracy: 0.9006\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 39s 865us/sample - loss: 0.4906 - accuracy: 0.8644 - val_loss: 0.3184 - val_accuracy: 0.9176\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 41s 913us/sample - loss: 0.4326 - accuracy: 0.8840 - val_loss: 0.2627 - val_accuracy: 0.9315\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 44s 985us/sample - loss: 0.3870 - accuracy: 0.8974 - val_loss: 0.2622 - val_accuracy: 0.9330\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 41s 915us/sample - loss: 0.3555 - accuracy: 0.9061 - val_loss: 0.2338 - val_accuracy: 0.9404\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 38s 857us/sample - loss: 0.3506 - accuracy: 0.9089 - val_loss: 0.2247 - val_accuracy: 0.9420\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 40s 888us/sample - loss: 0.3146 - accuracy: 0.9170 - val_loss: 0.2227 - val_accuracy: 0.9438\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 38s 839us/sample - loss: 0.3046 - accuracy: 0.9195 - val_loss: 0.2111 - val_accuracy: 0.9491\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 33s 727us/sample - loss: 0.2851 - accuracy: 0.9250 - val_loss: 0.2086 - val_accuracy: 0.9468\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 32s 720us/sample - loss: 0.2741 - accuracy: 0.9278 - val_loss: 0.1869 - val_accuracy: 0.9515\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 31s 699us/sample - loss: 0.2623 - accuracy: 0.9314 - val_loss: 0.1895 - val_accuracy: 0.9518\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 32s 723us/sample - loss: 0.2492 - accuracy: 0.9355 - val_loss: 0.1692 - val_accuracy: 0.9546\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 32s 714us/sample - loss: 0.2310 - accuracy: 0.9399 - val_loss: 0.1757 - val_accuracy: 0.9554\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 32s 723us/sample - loss: 0.2288 - accuracy: 0.9403 - val_loss: 0.1722 - val_accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "44800/44800 [==============================] - 32s 707us/sample - loss: 0.2225 - accuracy: 0.9407 - val_loss: 0.1685 - val_accuracy: 0.9556\n",
      "Epoch 19/20\n",
      "44800/44800 [==============================] - 32s 723us/sample - loss: 0.2168 - accuracy: 0.9424 - val_loss: 0.1623 - val_accuracy: 0.9582\n",
      "Epoch 20/20\n",
      "44800/44800 [==============================] - 32s 721us/sample - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1616 - val_accuracy: 0.9569\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=[X_val, y_val],\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "- Momentum\n",
    "    - For SGD\n",
    "- Better optimisers\n",
    "    - Adam\n",
    "    - Compare some others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_42 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 139,846\n",
      "Trainable params: 136,178\n",
      "Non-trainable params: 3,668\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "for i in range(20):\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=SGD(momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 36s 813us/sample - loss: 1.1458 - accuracy: 0.6124 - val_loss: 0.5969 - val_accuracy: 0.8218\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 31s 683us/sample - loss: 0.6835 - accuracy: 0.7956 - val_loss: 0.3877 - val_accuracy: 0.8945\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 31s 691us/sample - loss: 0.5391 - accuracy: 0.8470 - val_loss: 0.3834 - val_accuracy: 0.8759\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 31s 691us/sample - loss: 0.4572 - accuracy: 0.8723 - val_loss: 0.2871 - val_accuracy: 0.9265\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 31s 702us/sample - loss: 0.3907 - accuracy: 0.8923 - val_loss: 0.2648 - val_accuracy: 0.9343\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 31s 686us/sample - loss: 0.3557 - accuracy: 0.9040 - val_loss: 0.2553 - val_accuracy: 0.9379\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 31s 682us/sample - loss: 0.3151 - accuracy: 0.9159 - val_loss: 0.2059 - val_accuracy: 0.9471\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 31s 686us/sample - loss: 0.2880 - accuracy: 0.9221 - val_loss: 0.2005 - val_accuracy: 0.9464\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 32s 707us/sample - loss: 0.2672 - accuracy: 0.9281 - val_loss: 0.1982 - val_accuracy: 0.9521\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 32s 718us/sample - loss: 0.2525 - accuracy: 0.9312 - val_loss: 0.2323 - val_accuracy: 0.9471\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 31s 701us/sample - loss: 0.2371 - accuracy: 0.9357 - val_loss: 0.1773 - val_accuracy: 0.9541\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 31s 700us/sample - loss: 0.2231 - accuracy: 0.9399 - val_loss: 0.1559 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 33s 748us/sample - loss: 0.2123 - accuracy: 0.9426 - val_loss: 0.1640 - val_accuracy: 0.9586\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 32s 709us/sample - loss: 0.1993 - accuracy: 0.9467 - val_loss: 0.1630 - val_accuracy: 0.9566\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 32s 703us/sample - loss: 0.1894 - accuracy: 0.9490 - val_loss: 0.1517 - val_accuracy: 0.9594\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 32s 716us/sample - loss: 0.1838 - accuracy: 0.9512 - val_loss: 0.1451 - val_accuracy: 0.9579\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 30s 679us/sample - loss: 0.1761 - accuracy: 0.9521 - val_loss: 0.1333 - val_accuracy: 0.9651\n",
      "Epoch 18/20\n",
      "44800/44800 [==============================] - 32s 709us/sample - loss: 0.1705 - accuracy: 0.9537 - val_loss: 0.1278 - val_accuracy: 0.9667\n",
      "Epoch 19/20\n",
      "44800/44800 [==============================] - 31s 693us/sample - loss: 0.1559 - accuracy: 0.9581 - val_loss: 0.1251 - val_accuracy: 0.9667\n",
      "Epoch 20/20\n",
      "44800/44800 [==============================] - 32s 715us/sample - loss: 0.1591 - accuracy: 0.9559 - val_loss: 0.1238 - val_accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=[X_val, y_val],\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_model(lr):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=[784]))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    for i in range(20):\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=SGD(lr=lr, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "lr_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"lr\": []}\n",
    "\n",
    "# grid search\n",
    "lr_grid = RandomizedSearchCV(act_model, param_grid, n_iter=4, cv=2, verbose=0)\n",
    "lr_grid_results = act_grid.fit(X_train, y_train,\n",
    "                               validation_data=[X_val, y_val],\n",
    "                               callbacks=[tensorboard, early_stopping],\n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "- Analyse adjusting learning rate\n",
    "- Learning rate schedling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in ANNs\n",
    "\n",
    "- Why large networks can overfit\n",
    "- l1, l2 loss and regularisation\n",
    "- Implement regularisation\n",
    "- Dropout, monte carlo dropout\n",
    "- Create a network with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_model = Sequential()\n",
    "\n",
    "drop_model.add(BatchNormalization(input_shape=[784]))\n",
    "drop_model.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "for i in range(20):\n",
    "    drop_model.add(Dense(50, activation=\"relu\"))\n",
    "    drop_model.add(BatchNormalization())\n",
    "    drop_model.add(Dropout(rate=0.2))\n",
    "\n",
    "drop_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "drop_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      " 1472/44800 [..............................] - ETA: 2:48 - loss: 2.9485 - accuracy: 0.0917"
     ]
    }
   ],
   "source": [
    "history = drop_model.fit(X_train, y_train, epochs=20,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks (CNN)\n",
    "\n",
    "- Problems with neural networks and images\n",
    "- Convolutions, filters\n",
    "- Convolutional layers, feature maps\n",
    "- Simple CNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training better CNNs\n",
    "\n",
    "- Stride, step\n",
    "- Pooling, max pooling\n",
    "- Better CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "- What is transfer learning\n",
    "- Use transfer learning with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of ANNs\n",
    "\n",
    "- Problems with ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines (SVMs)\n",
    "\n",
    "Train an SVM (with a chosen Kernel) and perform the same analyses as for ANNs. Interpret  and  discuss  your  results. Does the model overfit? How do they compare with ANNs? And why? How does the type of kernel (e.g.linear, RBF, etc.) impact on performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines\n",
    "\n",
    "- Hyperplane\n",
    "- Support vectors\n",
    "- Minimising lagrange multiplier (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "- Implementation of SVM on MNIST\n",
    "- Plot decision boundaries from lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic SVM\n",
    "\n",
    "- What is a probabilistic SVM?\n",
    "- SVM on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dual problem\n",
    "\n",
    "What is the dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "- Different representations\n",
    "- Kernels\n",
    "- Mercer's theorem\n",
    "- Gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel trick\n",
    "\n",
    "- What is the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVMs\n",
    "\n",
    "- How kernel SVMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial KSVM\n",
    "\n",
    "- What is the polynomial kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF) KSVM\n",
    "\n",
    "- What is the RBF kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid KSVM\n",
    "\n",
    "- What is the sigmoid kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising SVMs\n",
    "\n",
    "- Hinge loss\n",
    "- Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of SVMs\n",
    "\n",
    "- Problems with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of ANNs and SVMs\n",
    "\n",
    "- Comparison of neural networks and support vector machines.\n",
    "- Area under ROC curve\n",
    "- Precision, recall, accuracy...\n",
    "- TP, TN, FP, FN and rates for each\n",
    "- Confusion matrix\n",
    "- Metric\n",
    "- Validation\n",
    "- Cross-validation\n",
    "- Prediction\n",
    "- Inference\n",
    "- Interpretability\n",
    "- Sensitivity vs specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
