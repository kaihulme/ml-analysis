{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MLPClassifer' from 'sklearn.neural_network' (/home/kai/anaconda3/envs/gpu/lib/python3.9/site-packages/sklearn/neural_network/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bb887e9678c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MLPClassifer' from 'sklearn.neural_network' (/home/kai/anaconda3/envs/gpu/lib/python3.9/site-packages/sklearn/neural_network/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, roc_auc_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.activations import relu, selu\n",
    "from tensorflow.keras.layers import Activation, LeakyReLU, PReLU\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "- What is supervised learning?\n",
    "- Training\n",
    "- Fitting\n",
    "- Labels\n",
    "- Error / cost / objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Clasification is a type of \n",
    "\n",
    "- What is classification?\n",
    "- Different approaches\n",
    "  - Logistic regression - log loss\n",
    "  - ANNs, SVMs\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No free lunch\n",
    "\n",
    "- Model selection.\n",
    "- Parametric / nonparametric models.\n",
    "- Discriminative / generative modelling.\n",
    "\n",
    "A model is a simplified replica of the real world. This allows us to build and understand them with relative ease and use them to make predictions of future events. However these simplifations abstract away details, meaning the model cannot be a full representation of the real world and will therefore make mistakes. The abstraction level of your model creates a trade-off between performance and accuracy.\n",
    "\n",
    "No free lunch theorum states that no model is guarenteed to work for all problems, all you can do is test and evaluate each to find a best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "As a model is a abstraction, so are the patterns within it. It is easy to create a model to fit to data, with little effort you could create a model which accuratly predicts the training data's labels given the features. You could imagine this as creating a giant look-up table; when a training instance is given to the model it finds that combination of features in the table and returns the related label.\n",
    "\n",
    "But what happens when a new set of features are given to the model it hasn't seen before? It wont have the exact result for the data and so it will make a wrong prediction. This phenomona of a model learning too well from the data it was trained on is called overfitting. This happens when the model learns the pattern in your abstraction rather than the pattern of the real problem. \n",
    "\n",
    "The aim is to build a model which generalises to the problem you are trying to solve. This way you can make predictions on unseen data and see the same results as you did whilst training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, testing and validation\n",
    "\n",
    "Model training is the process of learning from data. In supervised learning we have labeled data and so we have a target to aim for. Data is given to the model and slowly it learns in different ways how to tune itself to reproduce the correct labels when given that data.\n",
    "\n",
    "As discussed above it is not enough to predict the data it has learned from. To evaluate a model's real-world performance a model must be tested on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "- explain cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with MNIST\n",
    "\n",
    "- The aim of classification with MNIST\n",
    "- Get the data\n",
    "- Recap analysis\n",
    "  - Size, shape, type\n",
    "  - Features\n",
    "  - Missing data, outliers etc.\n",
    "  - Target feature of dataset (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall look at the number of unique features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 unique digit types.\n",
    "\n",
    "We shall put the data into a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784), y shape: (70000,)\n",
      "Image shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "Now we shall have a look at the image data to see what form it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X[0]), np.max(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the minimum and maximum values in appears that pixel are of the standard 0-255 intensity form. \n",
    "\n",
    "It is beneficial to some machine learning methods for values to be normalised between 0-1, notably neural networks so we shall do this in the data preparation step.\n",
    "\n",
    "Finally we shall check the datatypes for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are of float type as expected but it may be beneficial to convert the string type labels to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "  - train, val test sets\n",
    "  - cross validation\n",
    "  - reshape / retype\n",
    "\n",
    "As stated earlier normalising the feature values will be beneficial for machine learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_full = X / 255.0\n",
    "print(np.min(X_full), np.max(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also now convert the string type features to integers for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "y_full = y.astype('int64')\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create a train, validation and test set before we proceed using sklearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "44800\n",
      "11200\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print(X_full.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss validation / training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing model training\n",
    "\n",
    "We shall use Tensorboard to visualise our model results as well as analyse training.\n",
    "\n",
    "We shall configure it to create a new subdirectory for each model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tb_dir(name):\n",
    "    curr_dir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "    tb_dir = time.strftime(\"{}_model_%Y_%m_%d-%H-%M-%S\".format(name))\n",
    "    return os.path.join(curr_dir, tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a callback during model training for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(get_tb_dir(\"base\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with the callback will then write the logs to it's own directory in `tensorboard_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 4s 2ms/step - loss: 1.2350 - accuracy: 0.6550 - val_loss: 0.3782 - val_accuracy: 0.8952\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.3323 - accuracy: 0.9084 - val_loss: 0.3063 - val_accuracy: 0.9127\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.2687 - accuracy: 0.9235 - val_loss: 0.2719 - val_accuracy: 0.9234\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.2383 - accuracy: 0.9340 - val_loss: 0.2465 - val_accuracy: 0.9273\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.2090 - accuracy: 0.9418 - val_loss: 0.2280 - val_accuracy: 0.9334\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1911 - accuracy: 0.9449 - val_loss: 0.2107 - val_accuracy: 0.9375\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1762 - accuracy: 0.9501 - val_loss: 0.1997 - val_accuracy: 0.9395\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1594 - accuracy: 0.9549 - val_loss: 0.1885 - val_accuracy: 0.9421\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1513 - accuracy: 0.9563 - val_loss: 0.1795 - val_accuracy: 0.9474\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1405 - accuracy: 0.9598 - val_loss: 0.1724 - val_accuracy: 0.9487\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the Tensorboard logs in `/tensorboard_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 1s 2ms/step - loss: 0.1676 - accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1676267385482788, 0.9511428475379944]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning rate\n",
    "\n",
    "I shall create a Keras wrapper in order to use Scikit's grid search to analyse learning rate changes.\n",
    "\n",
    "With this wrapper we can search for params: \n",
    "- `kernel_initializer`: weigh and bias initialisation method\n",
    "- `optimizer`: optimisation method used in back progogation\n",
    "- `activation`: activation function used in forward pass\n",
    "- `lr`: learning rate of the optimiser\n",
    "- `momentum`: momentum of the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_model(lr):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "lr_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"lr\": [1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "# grid search\n",
    "lr_grid = GridSearchCV(act_model, param_grid, cv=2, verbose=0)\n",
    "lr_grid_results = act_grid.fit(X_train, y_train,\n",
    "                               validation_data=[X_val, y_val],\n",
    "                               callbacks=[TensorBoard(get_tb_dir(\"learning_rate\"))],\n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning rate scheduling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / exploding gradient problem\n",
    "\n",
    "- What is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines (SVMs)\n",
    "\n",
    "Train an SVM (with a chosen Kernel) and perform the same analyses as for ANNs. Interpret  and  discuss  your  results. Does the model overfit? How do they compare with ANNs? And why? How does the type of kernel (e.g.linear, RBF, etc.) impact on performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search soft-margin (C) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_linear_svm = LinearSVC(loss=\"squared_hinge\", dual=False,\n",
    "                            max_iter=1000)\n",
    "\n",
    "c_params = {\"C\": [0.01, 0.1, 1, 10, 10]}\n",
    "\n",
    "c_gridsearch = GridSearchCV(grid_linear_svm, c_params,\n",
    "                            cv=2, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "[CV] C=0.01 ..........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. C=0.01, score=0.903, total=   1.8s\n",
      "[CV] C=0.01 ..........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. C=0.01, score=0.904, total=   1.7s\n",
      "[CV] C=0.1 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................... C=0.1, score=0.898, total=   2.1s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.903, total=   2.2s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.883, total=   3.3s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.887, total=   3.5s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.866, total=   7.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.868, total=   8.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.866, total=   7.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.868, total=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   45.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=LinearSVC(dual=False),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10, 10]}, verbose=3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035978835978836"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial KSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_params = {\"degree\": [2, 3]}\n",
    "grid_poly = SVC(kernel='poly', max_iter=1000)\n",
    "poly_gridsearch = GridSearchCV(grid_poly, poly_params, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=SVC(kernel='poly', max_iter=1000),\n",
       "             param_grid={'degree': [2, 3]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544973544973545"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the 2nd degree polynomial kernel is a good fit, a 5% increase over the standard polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF) KSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_params = {\"gamma\": [1, 5, 'scale', 'auto']}\n",
    "grid_rbf = SVC(kernel='rbf', max_iter=5000)\n",
    "rbf_gridsearch = GridSearchCV(grid_rbf, rbf_params, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=SVC(max_iter=5000),\n",
       "             param_grid={'gamma': [1, 5, 'scale', 'auto']})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 'scale'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9579365079365079"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid KSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_params = {\"gamma\": [1, 5, 10, 'scale', 'auto']}\n",
    "grid_sigmoid = SVC(kernel='sigmoid', max_iter=5000)\n",
    "sigmoid_gridsearch = GridSearchCV(grid_sigmoid, sigmoid_params, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=SVC(kernel='sigmoid', max_iter=5000),\n",
       "             param_grid={'gamma': [1, 5, 10, 'scale', 'auto']})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 'auto'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9051851851851852"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = SVC(kernel=\"linear\").fit(X_train, y_train)\n",
    "poly_svm = SVC(kernel=\"poly\", max_iter=5000, degree=2).fit(X_train, y_train)\n",
    "rbf_svm = SVC(kernel=\"rbf\", max_iter=5000, gamma=\"scale\").fit(X_train, y_train)\n",
    "sigmoid_svm = SVC(kernel=\"sigmoid\", max_iter=5000, gamma=\"auto\").fit(X_train, y_train)\n",
    "\n",
    "svms = [\n",
    "    (\"linear\", linear_svm),\n",
    "    (\"polynomial\", poly_svm),\n",
    "    (\"rbf\", rbf_svm),\n",
    "    (\"sigmoid\", sigmoid_svm),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel: 0.9312142857142857\n",
      "polynomial kernel: 0.9713571428571428\n",
      "rbf kernel: 0.9751428571428571\n",
      "sigmoid kernel: 0.9242857142857143\n"
     ]
    }
   ],
   "source": [
    "for (name, svm) in svms:\n",
    "    print(f\"{name} kernel: {svm.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compairson of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svm on pca(2)\n",
    "- sklearn nn on pca(2)\n",
    "- compare decision boundaries\n",
    "- discuss pros and cons of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca(2) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02 \n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "cm = plt.cm.RdBu\n",
    "\n",
    "def Z_mesh(clf):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]        \n",
    "    return Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    MLPClassifier(alpha=1, max_iter=1000),    \n",
    "    SVC(kernel=\"scale\", gamma=\"auto\")\n",
    "]\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X, y)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.contourf(xx, yy, Z_mesh(clf), cmap=cm)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, edgecolors='k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
