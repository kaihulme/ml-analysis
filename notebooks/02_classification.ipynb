{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.activations import relu, selu\n",
    "from tensorflow.keras.layers import Activation, LeakyReLU, PReLU\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "- What is supervised learning?\n",
    "- Training\n",
    "- Fitting\n",
    "- Labels\n",
    "- Error / cost / objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "- What is classification?\n",
    "- Different approaches\n",
    "  - Logistic regression - log loss\n",
    "  - ANNs, SVMs\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No free lunch\n",
    "\n",
    "- Model selection.\n",
    "- Parametric / nonparametric models.\n",
    "- Discriminative / generative modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with MNIST\n",
    "\n",
    "- The aim of classification with MNIST\n",
    "- Get the data\n",
    "- Recap analysis\n",
    "  - Size, shape, type\n",
    "  - Features\n",
    "  - Missing data, outliers etc.\n",
    "  - Target feature of dataset (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall look at the number of unique features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 unique digit types.\n",
    "\n",
    "We shall put the data into a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784), y shape: (70000,)\n",
      "Image shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "Now we shall have a look at the image data to see what form it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X[0]), np.max(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the minimum and maximum values in appears that pixel are of the standard 0-255 intensity form. \n",
    "\n",
    "It is beneficial to some machine learning methods for values to be normalised between 0-1, notably neural networks so we shall do this in the data preparation step.\n",
    "\n",
    "Finally we shall check the datatypes for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are of float type as expected but it may be beneficial to convert the string type labels to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "  - train, val test sets\n",
    "  - cross validation\n",
    "  - reshape / retype\n",
    "\n",
    "As stated earlier normalising the feature values will be beneficial for machine learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_full = X / 255.0\n",
    "print(np.min(X_full), np.max(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also now convert the string type features to integers for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "y_full = y.astype('int64')\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create a train, validation and test set before we proceed using sklearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "44800\n",
      "11200\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print(X_full.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "- Neurons structure\n",
    "- Action potentials ~ activation functions\n",
    "- Weights, connections etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "\n",
    "- Perceptron history\n",
    "- Input layer\n",
    "- Output layer\n",
    "- Activation function\n",
    "- Weights\n",
    "- Bias\n",
    "- Try and apply it to MNIST (lots of neurons?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8815178571428571"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron regularisation\n",
    "\n",
    "This is a reasonably good result for such a simple model, but this is due to the simplicity of the data set.\n",
    "\n",
    "One way we can try and improve the results is through regularisation.\n",
    "\n",
    "** **EXPLAIN REGULARISATION** **\n",
    "\n",
    "As the model training doesn't take too long we shall try a grid search with `None`, `l1`, `l2` and `ElastiNet` regularisation.\n",
    "\n",
    "We shall do this with 5-fold cross validation, evaluating the accuracy on the validation set.\n",
    "\n",
    "** **EXPLAIN CROSS VALIDATION** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.866, test=0.848), total=   2.2s\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.883, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... penalty=None, score=(train=0.876, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n",
      "[CV] .... penalty=None, score=(train=0.885, test=0.870), total=   2.1s\n",
      "[CV] penalty=None ....................................................\n",
      "[CV] .... penalty=None, score=(train=0.853, test=0.841), total=   2.2s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.877, test=0.860), total=   2.2s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.885, test=0.877), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.872, test=0.865), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.885, test=0.875), total=   2.3s\n",
      "[CV] penalty=l1 ......................................................\n",
      "[CV] ...... penalty=l1, score=(train=0.863, test=0.853), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.851, test=0.841), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.851, test=0.849), total=   2.2s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.817, test=0.816), total=   2.3s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.853, test=0.847), total=   2.2s\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] ...... penalty=l2, score=(train=0.801, test=0.798), total=   2.2s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.851, test=0.841), total=   2.4s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.851, test=0.849), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.817, test=0.816), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.853, test=0.847), total=   2.3s\n",
      "[CV] penalty=elasticnet ..............................................\n",
      "[CV]  penalty=elasticnet, score=(train=0.801, test=0.798), total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Perceptron(early_stopping=True, n_jobs=-1,\n",
       "                                  random_state=42),\n",
       "             param_grid=[{'penalty': ['None', 'l1', 'l2', 'elasticnet']}],\n",
       "             return_train_score=True, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_perceptron = Perceptron(alpha=0.0001, tol=0.001,\n",
    "                            max_iter=1000, early_stopping=True,\n",
    "                            validation_fraction=0.1, n_iter_no_change=5,\n",
    "                            random_state=42, verbose=0, n_jobs=-1)\n",
    "\n",
    "perceptron_params = [{'penalty': ['None', 'l1', 'l2', 'elasticnet']}]\n",
    "\n",
    "grid_search = GridSearchCV(reg_perceptron, perceptron_params,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           return_train_score=True,\n",
    "                           verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the optimal parameters of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l1'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears l1 regularisation has produced the best results for the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "- Multiple layers of neurons\n",
    "- Able to learn more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPs\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Epochs, convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "- Calculation of outputs from input\n",
    "- Calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "- Gradient descent\n",
    "- Learning rate\n",
    "- Minimise loss function\n",
    "- Convex optimisation problem\n",
    "- Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- What is SGD\n",
    "- Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "- What is cross entropy\n",
    "- Sparse categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST\n",
    "\n",
    "- Feed forward network\n",
    "- Dense layers\n",
    "- Epochs, convergence\n",
    "- Create a simple MLP for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 9s 190us/sample - loss: 0.7054 - accuracy: 0.8126 - val_loss: 0.3659 - val_accuracy: 0.8951\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 7s 167us/sample - loss: 0.3244 - accuracy: 0.9065 - val_loss: 0.2966 - val_accuracy: 0.9142\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 183us/sample - loss: 0.2734 - accuracy: 0.9206 - val_loss: 0.2621 - val_accuracy: 0.9229\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.2413 - accuracy: 0.9305 - val_loss: 0.2313 - val_accuracy: 0.9306\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 9s 193us/sample - loss: 0.2161 - accuracy: 0.9379 - val_loss: 0.2104 - val_accuracy: 0.9385\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 168us/sample - loss: 0.1953 - accuracy: 0.9432 - val_loss: 0.1974 - val_accuracy: 0.9411\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 9s 206us/sample - loss: 0.1783 - accuracy: 0.9484 - val_loss: 0.1841 - val_accuracy: 0.9458\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.1633 - accuracy: 0.9522 - val_loss: 0.1700 - val_accuracy: 0.9489\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 9s 193us/sample - loss: 0.1501 - accuracy: 0.9563 - val_loss: 0.1669 - val_accuracy: 0.9489\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 9s 191us/sample - loss: 0.1388 - accuracy: 0.9601 - val_loss: 0.1513 - val_accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing model training\n",
    "\n",
    "We shall use Tensorboard to visualise our model results as well as analyse training.\n",
    "\n",
    "We shall configure it to create a new subdirectory for each model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tb_dir():\n",
    "    curr_dir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "    tb_dir = time.strftime(\"model_%Y_%m_%d-%H-%M-%S\")\n",
    "    return os.path.join(curr_dir, tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a callback during model training for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(get_tb_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with the callback will then write the logs to it's own directory in `tensorboard_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 8s 179us/sample - loss: 0.7610 - accuracy: 0.8044 - val_loss: 0.3651 - val_accuracy: 0.8956\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 8s 185us/sample - loss: 0.3232 - accuracy: 0.9075 - val_loss: 0.2893 - val_accuracy: 0.9163\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 180us/sample - loss: 0.2702 - accuracy: 0.9229 - val_loss: 0.2627 - val_accuracy: 0.9226\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 170us/sample - loss: 0.2387 - accuracy: 0.9313 - val_loss: 0.2281 - val_accuracy: 0.9337\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 8s 174us/sample - loss: 0.2151 - accuracy: 0.9384 - val_loss: 0.2086 - val_accuracy: 0.9376\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 183us/sample - loss: 0.1963 - accuracy: 0.9435 - val_loss: 0.1923 - val_accuracy: 0.9430\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 9s 199us/sample - loss: 0.1788 - accuracy: 0.9485 - val_loss: 0.1820 - val_accuracy: 0.9454\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 9s 204us/sample - loss: 0.1644 - accuracy: 0.9526 - val_loss: 0.1653 - val_accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 9s 206us/sample - loss: 0.1517 - accuracy: 0.9566 - val_loss: 0.1592 - val_accuracy: 0.9533\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 10s 216us/sample - loss: 0.1404 - accuracy: 0.9595 - val_loss: 0.1531 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !kill 132185\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=./tensorboard_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper neural networks\n",
    "\n",
    "- Create a large overfitting network\n",
    "- Early stopping, check pointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 382,510\n",
      "Trainable params: 382,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "for i in range(30):\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this large model with almost 400,000 parameters was a lot harder to train and generalised far worse on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / exploding gradient problem\n",
    "\n",
    "- What is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shall create a Keras wrapper in order to use Scikit's grid search.\n",
    "\n",
    "With this wrapper we can search for params: \n",
    "- `kernel_initializer`: weigh and bias initialisation method\n",
    "- `optimizer`: optimisation method used in back progogation\n",
    "- `activation`: activation function used in forward pass\n",
    "- `lr`: learning rate of the optimiser\n",
    "- `momentum`: momentum of the optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initlialisation\n",
    "\n",
    "- GloRoot\n",
    "- He\n",
    "- LeCeun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_init_model(init):\n",
    "    # Sequential model\n",
    "    model = Sequential()\n",
    "    # Input layer with activation layer\n",
    "    model.add(Dense(100, kernel_input_shape=[784]))\n",
    "    model.add(get_activation(activation))\n",
    "    # Hidden layers with activation function\n",
    "    for i in range(21):\n",
    "        model.add(Dense(50))\n",
    "        model.add(get_activation(activation))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = KerasClassifier(build_fn=create_init_model, epochs=10)\n",
    "param_grid = {\"init\": ['lecun_uniform', 'glorot_normal', 'glorot_uniform',\n",
    "                       'he_normal', 'he_uniform']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-saturating activation functions\n",
    "\n",
    "- Leaky RELU\n",
    "- SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return activation function as layer\n",
    "def get_activation(activation):\n",
    "    if (activation == 'relu'):\n",
    "        return Activation(relu)\n",
    "    elif (activation == 'selu'):\n",
    "        return Activation(selu)\n",
    "    elif (activation == 'leakyrelu'):\n",
    "        return LeakyReLU()\n",
    "    elif (activation == 'prelu'):\n",
    "        return PReLU()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "    # Sequential model\n",
    "    model = Sequential()\n",
    "    # Input layer with activation layer\n",
    "    model.add(Dense(100, input_shape=[784]))\n",
    "    model.add(get_activation(activation))\n",
    "    # Hidden layers with activation function\n",
    "    for i in range(21):\n",
    "        model.add(Dense(50))\n",
    "        model.add(get_activation(activation))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "act_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"activation\": ['relu', 'selu', 'leakyrelu', 'prelu']}\n",
    "\n",
    "# grid search\n",
    "act_grid = RandomizedSearchCV(act_model, param_grid, n_iter=4, cv=2, verbose=0)\n",
    "act_grid_results = act_grid.fit(X_train, y_train,\n",
    "                                validation_data=[X_val, y_val],\n",
    "                                callbacks=[tensorboard, early_stopping],\n",
    "                                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'prelu'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_grid_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "- Batch normalisation\n",
    "- Mini-batch\n",
    "- Implement batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_21 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 139,846\n",
      "Trainable params: 136,178\n",
      "Non-trainable params: 3,668\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "for i in range(20):\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 46s 1ms/sample - loss: 1.4566 - accuracy: 0.5210 - val_loss: 0.8362 - val_accuracy: 0.7379\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 43s 956us/sample - loss: 0.8345 - accuracy: 0.7538 - val_loss: 0.4975 - val_accuracy: 0.8545\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 39s 861us/sample - loss: 0.6466 - accuracy: 0.8164 - val_loss: 0.4047 - val_accuracy: 0.8841\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 40s 886us/sample - loss: 0.5597 - accuracy: 0.8455 - val_loss: 0.3630 - val_accuracy: 0.9006\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 39s 865us/sample - loss: 0.4906 - accuracy: 0.8644 - val_loss: 0.3184 - val_accuracy: 0.9176\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 41s 913us/sample - loss: 0.4326 - accuracy: 0.8840 - val_loss: 0.2627 - val_accuracy: 0.9315\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 44s 985us/sample - loss: 0.3870 - accuracy: 0.8974 - val_loss: 0.2622 - val_accuracy: 0.9330\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 41s 915us/sample - loss: 0.3555 - accuracy: 0.9061 - val_loss: 0.2338 - val_accuracy: 0.9404\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 38s 857us/sample - loss: 0.3506 - accuracy: 0.9089 - val_loss: 0.2247 - val_accuracy: 0.9420\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 40s 888us/sample - loss: 0.3146 - accuracy: 0.9170 - val_loss: 0.2227 - val_accuracy: 0.9438\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 38s 839us/sample - loss: 0.3046 - accuracy: 0.9195 - val_loss: 0.2111 - val_accuracy: 0.9491\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 33s 727us/sample - loss: 0.2851 - accuracy: 0.9250 - val_loss: 0.2086 - val_accuracy: 0.9468\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 32s 720us/sample - loss: 0.2741 - accuracy: 0.9278 - val_loss: 0.1869 - val_accuracy: 0.9515\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 31s 699us/sample - loss: 0.2623 - accuracy: 0.9314 - val_loss: 0.1895 - val_accuracy: 0.9518\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 32s 723us/sample - loss: 0.2492 - accuracy: 0.9355 - val_loss: 0.1692 - val_accuracy: 0.9546\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 32s 714us/sample - loss: 0.2310 - accuracy: 0.9399 - val_loss: 0.1757 - val_accuracy: 0.9554\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 32s 723us/sample - loss: 0.2288 - accuracy: 0.9403 - val_loss: 0.1722 - val_accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "44800/44800 [==============================] - 32s 707us/sample - loss: 0.2225 - accuracy: 0.9407 - val_loss: 0.1685 - val_accuracy: 0.9556\n",
      "Epoch 19/20\n",
      "44800/44800 [==============================] - 32s 723us/sample - loss: 0.2168 - accuracy: 0.9424 - val_loss: 0.1623 - val_accuracy: 0.9582\n",
      "Epoch 20/20\n",
      "44800/44800 [==============================] - 32s 721us/sample - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1616 - val_accuracy: 0.9569\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=[X_val, y_val],\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "- Momentum\n",
    "    - For SGD\n",
    "- Better optimisers\n",
    "    - Adam\n",
    "    - Compare some others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_42 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 139,846\n",
      "Trainable params: 136,178\n",
      "Non-trainable params: 3,668\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "for i in range(20):\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=SGD(momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 36s 813us/sample - loss: 1.1458 - accuracy: 0.6124 - val_loss: 0.5969 - val_accuracy: 0.8218\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 31s 683us/sample - loss: 0.6835 - accuracy: 0.7956 - val_loss: 0.3877 - val_accuracy: 0.8945\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 31s 691us/sample - loss: 0.5391 - accuracy: 0.8470 - val_loss: 0.3834 - val_accuracy: 0.8759\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 31s 691us/sample - loss: 0.4572 - accuracy: 0.8723 - val_loss: 0.2871 - val_accuracy: 0.9265\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 31s 702us/sample - loss: 0.3907 - accuracy: 0.8923 - val_loss: 0.2648 - val_accuracy: 0.9343\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 31s 686us/sample - loss: 0.3557 - accuracy: 0.9040 - val_loss: 0.2553 - val_accuracy: 0.9379\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 31s 682us/sample - loss: 0.3151 - accuracy: 0.9159 - val_loss: 0.2059 - val_accuracy: 0.9471\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 31s 686us/sample - loss: 0.2880 - accuracy: 0.9221 - val_loss: 0.2005 - val_accuracy: 0.9464\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 32s 707us/sample - loss: 0.2672 - accuracy: 0.9281 - val_loss: 0.1982 - val_accuracy: 0.9521\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 32s 718us/sample - loss: 0.2525 - accuracy: 0.9312 - val_loss: 0.2323 - val_accuracy: 0.9471\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 31s 701us/sample - loss: 0.2371 - accuracy: 0.9357 - val_loss: 0.1773 - val_accuracy: 0.9541\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 31s 700us/sample - loss: 0.2231 - accuracy: 0.9399 - val_loss: 0.1559 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 33s 748us/sample - loss: 0.2123 - accuracy: 0.9426 - val_loss: 0.1640 - val_accuracy: 0.9586\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 32s 709us/sample - loss: 0.1993 - accuracy: 0.9467 - val_loss: 0.1630 - val_accuracy: 0.9566\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 32s 703us/sample - loss: 0.1894 - accuracy: 0.9490 - val_loss: 0.1517 - val_accuracy: 0.9594\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 32s 716us/sample - loss: 0.1838 - accuracy: 0.9512 - val_loss: 0.1451 - val_accuracy: 0.9579\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 30s 679us/sample - loss: 0.1761 - accuracy: 0.9521 - val_loss: 0.1333 - val_accuracy: 0.9651\n",
      "Epoch 18/20\n",
      "44800/44800 [==============================] - 32s 709us/sample - loss: 0.1705 - accuracy: 0.9537 - val_loss: 0.1278 - val_accuracy: 0.9667\n",
      "Epoch 19/20\n",
      "44800/44800 [==============================] - 31s 693us/sample - loss: 0.1559 - accuracy: 0.9581 - val_loss: 0.1251 - val_accuracy: 0.9667\n",
      "Epoch 20/20\n",
      "44800/44800 [==============================] - 32s 715us/sample - loss: 0.1591 - accuracy: 0.9559 - val_loss: 0.1238 - val_accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=[X_val, y_val],\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive moment esitmation (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = Sequential()\n",
    "adam_model.add(Dense(100, activation=\"relu\"))\n",
    "adam_model.add(BatchNormalization(input_shape=[784]))\n",
    "for i in range(20):\n",
    "    adam_model.add(Dense(50, activation=\"relu\"))\n",
    "    adam_model.add(BatchNormalization())\n",
    "adam_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "adam_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 45s 994us/sample - loss: 1.8571 - accuracy: 0.3513 - val_loss: 1.0240 - val_accuracy: 0.6459\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 40s 892us/sample - loss: 0.9662 - accuracy: 0.6927 - val_loss: 0.4787 - val_accuracy: 0.8678\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 38s 856us/sample - loss: 0.6507 - accuracy: 0.8150 - val_loss: 0.3270 - val_accuracy: 0.9131\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 39s 881us/sample - loss: 0.5140 - accuracy: 0.8610 - val_loss: 0.2956 - val_accuracy: 0.9269\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 38s 859us/sample - loss: 0.4149 - accuracy: 0.8914 - val_loss: 0.2656 - val_accuracy: 0.9362\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 38s 858us/sample - loss: 0.3666 - accuracy: 0.9047 - val_loss: 0.2423 - val_accuracy: 0.9437\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 40s 883us/sample - loss: 0.3242 - accuracy: 0.9170 - val_loss: 0.2368 - val_accuracy: 0.9457\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 39s 881us/sample - loss: 0.2930 - accuracy: 0.9253 - val_loss: 0.2280 - val_accuracy: 0.9504\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 39s 870us/sample - loss: 0.2641 - accuracy: 0.9327 - val_loss: 0.2139 - val_accuracy: 0.9440\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 40s 895us/sample - loss: 0.2351 - accuracy: 0.9416 - val_loss: 0.1787 - val_accuracy: 0.9559\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 41s 915us/sample - loss: 0.2143 - accuracy: 0.9445 - val_loss: 0.1692 - val_accuracy: 0.9574\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 40s 885us/sample - loss: 0.2032 - accuracy: 0.9483 - val_loss: 0.2183 - val_accuracy: 0.9514\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 41s 909us/sample - loss: 0.1926 - accuracy: 0.9514 - val_loss: 0.1740 - val_accuracy: 0.9606\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 42s 948us/sample - loss: 0.1721 - accuracy: 0.9569 - val_loss: 0.1396 - val_accuracy: 0.9651\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 41s 924us/sample - loss: 0.1686 - accuracy: 0.9572 - val_loss: 0.1552 - val_accuracy: 0.9630\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 41s 922us/sample - loss: 0.1554 - accuracy: 0.9609 - val_loss: 0.1354 - val_accuracy: 0.9673\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 80s 2ms/sample - loss: 0.1426 - accuracy: 0.9635 - val_loss: 0.1458 - val_accuracy: 0.9634\n",
      "Epoch 18/20\n",
      "37664/44800 [========================>.....] - ETA: 1:11 - loss: 0.1381 - accuracy: 0.96 - ETA: 1:22 - loss: 0.1384 - accuracy: 0.96 - ETA: 1:36 - loss: 0.1384 - accuracy: 0.9647"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "history = adam_model.fit(X_train, y_train, epochs=20,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Adam optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2799/2800 [============================>.] - ETA: 0s - loss: 1.9125 - accuracy: 0.3221"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:909 test_step  **\n        y_pred = self(x, training=False)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:155 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer sequential expects 1 inputs, but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 784) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(16, 1) dtype=int64>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5e75c5f8f336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = adam_model.fit(X_train, y_train, epochs=20, batch_size=16,\n\u001b[0m\u001b[1;32m      4\u001b[0m                          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          callbacks=[tensorboard, early_stopping])\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:909 test_step  **\n        y_pred = self(x, training=False)\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /home/kai/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:155 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer sequential expects 1 inputs, but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 784) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(16, 1) dtype=int64>]\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "history = adam_model.fit(X_train, y_train, epochs=20, batch_size=16,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_model(lr):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=[784]))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    for i in range(20):\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=SGD(lr=lr, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "lr_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"lr\": []}\n",
    "\n",
    "# grid search\n",
    "lr_grid = RandomizedSearchCV(act_model, param_grid, n_iter=4, cv=2, verbose=0)\n",
    "lr_grid_results = act_grid.fit(X_train, y_train,\n",
    "                               validation_data=[X_val, y_val],\n",
    "                               callbacks=[tensorboard, early_stopping],\n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "- Analyse adjusting learning rate\n",
    "- Learning rate schedling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in ANNs\n",
    "\n",
    "- Why large networks can overfit\n",
    "- l1, l2 loss and regularisation\n",
    "- Implement regularisation\n",
    "- Dropout, monte carlo dropout\n",
    "- Create a network with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_model = Sequential()\n",
    "\n",
    "drop_model.add(BatchNormalization(input_shape=[784]))\n",
    "drop_model.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "for i in range(20):\n",
    "    drop_model.add(Dense(50, activation=\"relu\"))\n",
    "    drop_model.add(BatchNormalization())\n",
    "\n",
    "drop_model.add(Dense(50, activation=\"relu\"))\n",
    "drop_model.add(Dropout(rate=0.5))\n",
    "\n",
    "drop_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "drop_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/20\n",
      "44800/44800 [==============================] - 40s 892us/sample - loss: 1.7954 - accuracy: 0.3976 - val_loss: 1.0093 - val_accuracy: 0.6487\n",
      "Epoch 2/20\n",
      "44800/44800 [==============================] - 33s 746us/sample - loss: 1.1686 - accuracy: 0.6336 - val_loss: 0.7085 - val_accuracy: 0.7703\n",
      "Epoch 3/20\n",
      "44800/44800 [==============================] - 33s 726us/sample - loss: 0.9309 - accuracy: 0.7276 - val_loss: 0.5051 - val_accuracy: 0.8550\n",
      "Epoch 4/20\n",
      "44800/44800 [==============================] - 33s 733us/sample - loss: 0.7723 - accuracy: 0.7997 - val_loss: 0.3993 - val_accuracy: 0.8919\n",
      "Epoch 5/20\n",
      "44800/44800 [==============================] - 33s 731us/sample - loss: 0.6710 - accuracy: 0.8310 - val_loss: 0.3502 - val_accuracy: 0.9045\n",
      "Epoch 6/20\n",
      "44800/44800 [==============================] - 32s 721us/sample - loss: 0.5939 - accuracy: 0.8524 - val_loss: 0.3058 - val_accuracy: 0.9213\n",
      "Epoch 7/20\n",
      "44800/44800 [==============================] - 33s 738us/sample - loss: 0.5568 - accuracy: 0.8675 - val_loss: 0.2905 - val_accuracy: 0.9254\n",
      "Epoch 8/20\n",
      "44800/44800 [==============================] - 33s 731us/sample - loss: 0.5109 - accuracy: 0.8784 - val_loss: 0.2833 - val_accuracy: 0.9298\n",
      "Epoch 9/20\n",
      "44800/44800 [==============================] - 32s 724us/sample - loss: 0.4850 - accuracy: 0.8847 - val_loss: 0.2534 - val_accuracy: 0.9321\n",
      "Epoch 10/20\n",
      "44800/44800 [==============================] - 34s 749us/sample - loss: 0.4505 - accuracy: 0.8942 - val_loss: 0.2398 - val_accuracy: 0.9396\n",
      "Epoch 11/20\n",
      "44800/44800 [==============================] - 33s 732us/sample - loss: 0.4325 - accuracy: 0.8984 - val_loss: 0.2167 - val_accuracy: 0.9456\n",
      "Epoch 12/20\n",
      "44800/44800 [==============================] - 33s 731us/sample - loss: 0.3984 - accuracy: 0.9064 - val_loss: 0.2194 - val_accuracy: 0.9437\n",
      "Epoch 13/20\n",
      "44800/44800 [==============================] - 33s 733us/sample - loss: 0.3838 - accuracy: 0.9119 - val_loss: 0.2095 - val_accuracy: 0.9452\n",
      "Epoch 14/20\n",
      "44800/44800 [==============================] - 34s 751us/sample - loss: 0.3720 - accuracy: 0.9136 - val_loss: 0.2205 - val_accuracy: 0.9448\n",
      "Epoch 15/20\n",
      "44800/44800 [==============================] - 33s 736us/sample - loss: 0.3616 - accuracy: 0.9154 - val_loss: 0.1958 - val_accuracy: 0.9505\n",
      "Epoch 16/20\n",
      "44800/44800 [==============================] - 33s 741us/sample - loss: 0.3567 - accuracy: 0.9164 - val_loss: 0.2070 - val_accuracy: 0.9484\n",
      "Epoch 17/20\n",
      "44800/44800 [==============================] - 34s 753us/sample - loss: 0.3317 - accuracy: 0.9217 - val_loss: 0.1835 - val_accuracy: 0.9525\n",
      "Epoch 18/20\n",
      "44800/44800 [==============================] - 35s 782us/sample - loss: 0.3208 - accuracy: 0.9264 - val_loss: 0.1804 - val_accuracy: 0.9521\n",
      "Epoch 19/20\n",
      "44800/44800 [==============================] - 36s 799us/sample - loss: 0.3025 - accuracy: 0.9292 - val_loss: 0.1777 - val_accuracy: 0.9550\n",
      "Epoch 20/20\n",
      "44800/44800 [==============================] - 36s 794us/sample - loss: 0.3050 - accuracy: 0.9290 - val_loss: 0.1756 - val_accuracy: 0.9564\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "history = drop_model.fit(X_train, y_train, epochs=20,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks (CNN)\n",
    "\n",
    "- Problems with neural networks and images\n",
    "- Convolutions, filters\n",
    "- Convolutional layers, feature maps\n",
    "- Simple CNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training better CNNs\n",
    "\n",
    "- Stride, step\n",
    "- Pooling, max pooling\n",
    "- Better CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "- What is transfer learning\n",
    "- Use transfer learning with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of ANNs\n",
    "\n",
    "- Problems with ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines (SVMs)\n",
    "\n",
    "Train an SVM (with a chosen Kernel) and perform the same analyses as for ANNs. Interpret  and  discuss  your  results. Does the model overfit? How do they compare with ANNs? And why? How does the type of kernel (e.g.linear, RBF, etc.) impact on performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines\n",
    "\n",
    "- Hyperplane\n",
    "- Support vectors\n",
    "- Minimising lagrange multiplier (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "- Implementation of SVM on MNIST\n",
    "- Plot decision boundaries from lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic SVM\n",
    "\n",
    "- What is a probabilistic SVM?\n",
    "- SVM on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dual problem\n",
    "\n",
    "What is the dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "- Different representations\n",
    "- Kernels\n",
    "- Mercer's theorem\n",
    "- Gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel trick\n",
    "\n",
    "- What is the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVMs\n",
    "\n",
    "- How kernel SVMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial KSVM\n",
    "\n",
    "- What is the polynomial kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF) KSVM\n",
    "\n",
    "- What is the RBF kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid KSVM\n",
    "\n",
    "- What is the sigmoid kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising SVMs\n",
    "\n",
    "- Hinge loss\n",
    "- Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of SVMs\n",
    "\n",
    "- Problems with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of ANNs and SVMs\n",
    "\n",
    "- Comparison of neural networks and support vector machines.\n",
    "- Area under ROC curve\n",
    "- Precision, recall, accuracy...\n",
    "- TP, TN, FP, FN and rates for each\n",
    "- Confusion matrix\n",
    "- Metric\n",
    "- Validation\n",
    "- Cross-validation\n",
    "- Prediction\n",
    "- Inference\n",
    "- Interpretability\n",
    "- Sensitivity vs specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
