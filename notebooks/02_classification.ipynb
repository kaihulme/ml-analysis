{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "Compare and contrast classification on the MNIST dataset with artificial neural networks (ANNs) and support vector machines (SVMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784), y shape: (70000,)\n",
      "Image shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X / 255)\n",
    "y = np.array(y, dtype='int')\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "We will need to create a train, validation and test set before we proceed using SKlearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "44800\n",
      "11200\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs)\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The original neural network came in the form of a perceptron..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Adding more layers to this we get a multi-layer perceptron:\n",
    "- DNN\n",
    "- chain rule\n",
    "- activation functions\n",
    "- layers\n",
    "- backpropogation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 4s 3ms/step - loss: 0.6903 - accuracy: 0.8213 - val_loss: 0.3466 - val_accuracy: 0.8991\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.3159 - accuracy: 0.9100 - val_loss: 0.2694 - val_accuracy: 0.9198\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.2600 - accuracy: 0.9252 - val_loss: 0.2393 - val_accuracy: 0.9301\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 4s 3ms/step - loss: 0.2237 - accuracy: 0.9356 - val_loss: 0.2081 - val_accuracy: 0.9381\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1956 - accuracy: 0.9433 - val_loss: 0.1880 - val_accuracy: 0.9433\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1738 - accuracy: 0.9501 - val_loss: 0.1700 - val_accuracy: 0.9488\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1560 - accuracy: 0.9548 - val_loss: 0.1567 - val_accuracy: 0.9526\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1417 - accuracy: 0.9592 - val_loss: 0.1450 - val_accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1291 - accuracy: 0.9632 - val_loss: 0.1383 - val_accuracy: 0.9596\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1186 - accuracy: 0.9661 - val_loss: 0.1350 - val_accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Networks (DNN)\n",
    "Tuning:\n",
    "- activation functions\n",
    "- optimsier \n",
    "- learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(400, activation=\"relu\"))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(25, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 1.0119 - accuracy: 0.6736 - val_loss: 0.3504 - val_accuracy: 0.8922\n",
      "Epoch 2/20\n",
      "1400/1400 [==============================] - 8s 6ms/step - loss: 0.2486 - accuracy: 0.9279 - val_loss: 0.1961 - val_accuracy: 0.9433\n",
      "Epoch 3/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.1622 - accuracy: 0.9531 - val_loss: 0.1517 - val_accuracy: 0.9548\n",
      "Epoch 4/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.1213 - accuracy: 0.9643 - val_loss: 0.1370 - val_accuracy: 0.9588\n",
      "Epoch 5/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0954 - accuracy: 0.9718 - val_loss: 0.1221 - val_accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.0756 - accuracy: 0.9772 - val_loss: 0.1135 - val_accuracy: 0.9665\n",
      "Epoch 7/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.0596 - accuracy: 0.9823 - val_loss: 0.1034 - val_accuracy: 0.9703\n",
      "Epoch 8/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0481 - accuracy: 0.9851 - val_loss: 0.1013 - val_accuracy: 0.9703\n",
      "Epoch 9/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.0379 - accuracy: 0.9888 - val_loss: 0.1006 - val_accuracy: 0.9725\n",
      "Epoch 10/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.1031 - val_accuracy: 0.9731\n",
      "Epoch 11/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0246 - accuracy: 0.9927 - val_loss: 0.1246 - val_accuracy: 0.9676\n",
      "Epoch 12/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.1557 - val_accuracy: 0.9602\n",
      "Epoch 13/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0145 - accuracy: 0.9963 - val_loss: 0.1182 - val_accuracy: 0.9707\n",
      "Epoch 14/20\n",
      "1400/1400 [==============================] - 13s 10ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.1062 - val_accuracy: 0.9754\n",
      "Epoch 15/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.1181 - val_accuracy: 0.9730\n",
      "Epoch 16/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.1109 - val_accuracy: 0.9765\n",
      "Epoch 17/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.1131 - val_accuracy: 0.9762\n",
      "Epoch 18/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.1170 - val_accuracy: 0.9766\n",
      "Epoch 19/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.1358 - val_accuracy: 0.9706\n",
      "Epoch 20/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.1208 - val_accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vanishing gradient problem\n",
    "- autodiff?\n",
    "- batch normalisation\n",
    "- momentum\n",
    "- compare optimisers\n",
    "- learning rate scheduling\n",
    "- l1, l2 regularisation\n",
    "- droupout, monte carlo dropout\n",
    "- max norm regularisation\n",
    "- tensorflow graphs / tensorboard\n",
    "- one-hot encoding MNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various examples from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "- problems with DNNs for images\n",
    "- convolutions, filters, convolutional layers\n",
    "- stacking feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- memory issues\n",
    "- pooling layers\n",
    "- dropout?\n",
    "- pretrained models?\n",
    "- fully convolutional network\n",
    "- mAP?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
