{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.activations import relu, selu\n",
    "from tensorflow.keras.layers import Activation, LeakyReLU, PReLU\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "- What is supervised learning?\n",
    "- Training\n",
    "- Fitting\n",
    "- Labels\n",
    "- Error / cost / objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "- What is classification?\n",
    "- Different approaches\n",
    "  - Logistic regression - log loss\n",
    "  - ANNs, SVMs\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No free lunch\n",
    "\n",
    "- Model selection.\n",
    "- Parametric / nonparametric models.\n",
    "- Discriminative / generative modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with MNIST\n",
    "\n",
    "- The aim of classification with MNIST\n",
    "- Get the data\n",
    "- Recap analysis\n",
    "  - Size, shape, type\n",
    "  - Features\n",
    "  - Missing data, outliers etc.\n",
    "  - Target feature of dataset (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall look at the number of unique features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 unique digit types.\n",
    "\n",
    "We shall put the data into a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "Now we shall have a look at the image data to see what form it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.min(X[0]), np.max(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the minimum and maximum values in appears that pixel are of the standard 0-255 intensity form. \n",
    "\n",
    "It is beneficial to some machine learning methods for values to be normalised between 0-1, notably neural networks so we shall do this in the data preparation step.\n",
    "\n",
    "Finally we shall check the datatypes for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are of float type as expected but it may be beneficial to convert the string type labels to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "  - train, val test sets\n",
    "  - cross validation\n",
    "  - reshape / retype\n",
    "\n",
    "As stated earlier normalising the feature values will be beneficial for machine learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_full = X / 255.0\n",
    "print(np.min(X_full), np.max(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also now convert the string type features to integers for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_full = y.astype('int64')\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create a train, validation and test set before we proceed using sklearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_full.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "- Neurons structure\n",
    "- Action potentials ~ activation functions\n",
    "- Weights, connections etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "\n",
    "- Perceptron history\n",
    "- Input layer\n",
    "- Output layer\n",
    "- Activation function\n",
    "- Weights\n",
    "- Bias\n",
    "- Try and apply it to MNIST (lots of neurons?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perceptron.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron regularisation\n",
    "\n",
    "This is a reasonably good result for such a simple model, but this is due to the simplicity of the data set.\n",
    "\n",
    "One way we can try and improve the results is through regularisation.\n",
    "\n",
    "** **EXPLAIN REGULARISATION** **\n",
    "\n",
    "As the model training doesn't take too long we shall try a grid search with `None`, `l1`, `l2` and `ElastiNet` regularisation.\n",
    "\n",
    "We shall do this with 5-fold cross validation, evaluating the accuracy on the validation set.\n",
    "\n",
    "** **EXPLAIN CROSS VALIDATION** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_perceptron = Perceptron(alpha=0.0001, tol=0.001,\n",
    "                            max_iter=1000, early_stopping=True,\n",
    "                            validation_fraction=0.1, n_iter_no_change=5,\n",
    "                            random_state=42, verbose=0, n_jobs=-1)\n",
    "\n",
    "perceptron_params = [{'penalty': ['None', 'l1', 'l2', 'elasticnet']}]\n",
    "\n",
    "grid_search = GridSearchCV(reg_perceptron, perceptron_params,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           return_train_score=True,\n",
    "                           verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the optimal parameters of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears l1 regularisation has produced the best results for the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "- Multiple layers of neurons\n",
    "- Able to learn more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPs\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Epochs, convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "- Calculation of outputs from input\n",
    "- Calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "- Gradient descent\n",
    "- Learning rate\n",
    "- Minimise loss function\n",
    "- Convex optimisation problem\n",
    "- Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- What is SGD\n",
    "- Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "- What is cross entropy\n",
    "- Sparse categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST\n",
    "\n",
    "- Feed forward network\n",
    "- Dense layers\n",
    "- Epochs, convergence\n",
    "- Create a simple MLP for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing model training\n",
    "\n",
    "We shall use Tensorboard to visualise our model results as well as analyse training.\n",
    "\n",
    "We shall configure it to create a new subdirectory for each model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tb_dir():\n",
    "    curr_dir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "    tb_dir = time.strftime(\"model_%Y_%m_%d-%H-%M-%S\")\n",
    "    return os.path.join(curr_dir, tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a callback during model training for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(get_tb_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with the callback will then write the logs to it's own directory in `tensorboard_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !kill 132185\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=./tensorboard_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper neural networks\n",
    "\n",
    "- Create a large overfitting network\n",
    "- Early stopping, check pointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_shape=[784]))\n",
    "for i in range(30):\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this large model with almost 400,000 parameters was a lot harder to train and generalised far worse on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / exploding gradient problem\n",
    "\n",
    "- What is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shall create a Keras wrapper in order to use Scikit's grid search.\n",
    "\n",
    "With this wrapper we can search for params: \n",
    "- `kernel_initializer`: weigh and bias initialisation method\n",
    "- `optimizer`: optimisation method used in back progogation\n",
    "- `activation`: activation function used in forward pass\n",
    "- `lr`: learning rate of the optimiser\n",
    "- `momentum`: momentum of the optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initlialisation\n",
    "\n",
    "- GloRoot\n",
    "- He\n",
    "- LeCeun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_init_model(init):\n",
    "    # Sequential model\n",
    "    model = Sequential()\n",
    "    # Input layer with activation layer\n",
    "    model.add(Dense(100, kernel_input_shape=[784]))\n",
    "    model.add(get_activation(activation))\n",
    "    # Hidden layers with activation function\n",
    "    for i in range(21):\n",
    "        model.add(Dense(50))\n",
    "        model.add(get_activation(activation))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = KerasClassifier(build_fn=create_init_model, epochs=10)\n",
    "param_grid = {\"init\": ['lecun_uniform', 'glorot_normal', 'glorot_uniform',\n",
    "                       'he_normal', 'he_uniform']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-saturating activation functions\n",
    "\n",
    "- Leaky RELU\n",
    "- SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return activation function as layer\n",
    "def get_activation(activation):\n",
    "    if (activation == 'relu'):\n",
    "        return Activation(relu)\n",
    "    elif (activation == 'selu'):\n",
    "        return Activation(selu)\n",
    "    elif (activation == 'leakyrelu'):\n",
    "        return LeakyReLU()\n",
    "    elif (activation == 'prelu'):\n",
    "        return PReLU()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "    # Sequential model\n",
    "    model = Sequential()\n",
    "    # Input layer with activation layer\n",
    "    model.add(Dense(100, input_shape=[784]))\n",
    "    model.add(get_activation(activation))\n",
    "    # Hidden layers with activation function\n",
    "    for i in range(21):\n",
    "        model.add(Dense(50))\n",
    "        model.add(get_activation(activation))\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "act_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"activation\": ['relu', 'selu', 'leakyrelu', 'prelu']}\n",
    "\n",
    "# grid search\n",
    "act_grid = RandomizedSearchCV(act_model, param_grid, n_iter=4, cv=2, verbose=0)\n",
    "act_grid_results = act_grid.fit(X_train, y_train,\n",
    "                                validation_data=[X_val, y_val],\n",
    "                                callbacks=[tensorboard, early_stopping],\n",
    "                                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_grid_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "- Batch normalisation\n",
    "- Mini-batch\n",
    "- Implement batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "for i in range(20):\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=[X_val, y_val],\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "- Momentum\n",
    "    - For SGD\n",
    "- Better optimisers\n",
    "    - Adam\n",
    "    - Compare some others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "for i in range(20):\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=SGD(momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=[X_val, y_val],\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive moment esitmation (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = Sequential()\n",
    "adam_model.add(Dense(100, activation=\"relu\"))\n",
    "adam_model.add(BatchNormalization(input_shape=[784]))\n",
    "for i in range(20):\n",
    "    adam_model.add(Dense(50, activation=\"relu\"))\n",
    "    adam_model.add(BatchNormalization())\n",
    "adam_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "adam_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "history = adam_model.fit(X_train, y_train, epochs=20,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Adam optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "history = adam_model.fit(X_train, y_train, epochs=20, batch_size=16,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_model(lr):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=[784]))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    for i in range(20):\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=SGD(lr=lr, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "lr_model = KerasClassifier(build_fn=create_model, epochs=10)\n",
    "param_grid = {\"lr\": []}\n",
    "\n",
    "# grid search\n",
    "lr_grid = GridSearchCV(act_model, param_grid, cv=2, verbose=0)\n",
    "lr_grid_results = act_grid.fit(X_train, y_train,\n",
    "                               validation_data=[X_val, y_val],\n",
    "                               callbacks=[tensorboard, early_stopping],\n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "- Analyse adjusting learning rate\n",
    "- Learning rate schedling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in ANNs\n",
    "\n",
    "- Why large networks can overfit\n",
    "- l1, l2 loss and regularisation\n",
    "- Implement regularisation\n",
    "- Dropout, monte carlo dropout\n",
    "- Create a network with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_model = Sequential()\n",
    "\n",
    "drop_model.add(BatchNormalization(input_shape=[784]))\n",
    "drop_model.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "for i in range(20):\n",
    "    drop_model.add(Dense(50, activation=\"relu\"))\n",
    "    drop_model.add(BatchNormalization())\n",
    "\n",
    "drop_model.add(Dense(50, activation=\"relu\"))\n",
    "drop_model.add(Dropout(rate=0.5))\n",
    "\n",
    "drop_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "drop_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "history = drop_model.fit(X_train, y_train, epochs=20,\n",
    "                         validation_data=[X_val, y_val],\n",
    "                         callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks (CNN)\n",
    "\n",
    "- Problems with neural networks and images\n",
    "- Convolutions, filters\n",
    "- Convolutional layers, feature maps\n",
    "- Simple CNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training better CNNs\n",
    "\n",
    "- Stride, step\n",
    "- Pooling, max pooling\n",
    "- Better CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "- What is transfer learning\n",
    "- Use transfer learning with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of ANNs\n",
    "\n",
    "- Problems with ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines (SVMs)\n",
    "\n",
    "Train an SVM (with a chosen Kernel) and perform the same analyses as for ANNs. Interpret  and  discuss  your  results. Does the model overfit? How do they compare with ANNs? And why? How does the type of kernel (e.g.linear, RBF, etc.) impact on performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MNIST data and prepare for model\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise and scale\n",
    "X_full = X / 255.0\n",
    "X_full = StandardScaler().fit_transform(X_full)\n",
    "\n",
    "# convert type to int\n",
    "y_full = y.astype('int64')\n",
    "\n",
    "# reduce number of features\n",
    "# X_reduced = PCA(n_components=392).fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small training set for faster training with a test and val set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_full,\n",
    "                                                    test_size=0.7)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18900, 392)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0d82b72950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_full[0].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines\n",
    "\n",
    "- Hyperplane\n",
    "- Support vectors\n",
    "- Minimising lagrange multiplier (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "- Implementation of SVM on MNIST\n",
    "- Plot decision boundaries from lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit linear SVM to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC(loss=\"squared_hinge\", dual=False,\n",
    "                       max_iter=1000, verbose=10)\n",
    "linear_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = linear_svm.score(X_val, y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising SVMs\n",
    "\n",
    "- Hinge loss\n",
    "- Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dual problem\n",
    "\n",
    "What is the dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft-margin classification\n",
    "\n",
    "- What is soft-margin classification\n",
    "- What is the C parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_linear_svm = LinearSVC(loss=\"squared_hinge\", dual=False,\n",
    "                            max_iter=5000)\n",
    "\n",
    "c_params = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "c_gridsearch = GridSearchCV(grid_linear_svm, c_params,\n",
    "                            cv=3, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit grid search to train data with 3-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] C=0.001 .........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. C=0.001, score=0.897, total=   7.0s\n",
      "[CV] C=0.001 .........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. C=0.001, score=0.906, total=   6.3s\n",
      "[CV] C=0.001 .........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   13.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. C=0.001, score=0.906, total=   6.1s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.909, total=   8.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.914, total=   8.3s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.914, total=   8.1s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.908, total=  11.9s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.913, total=  12.1s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.915, total=  11.7s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.907, total=  21.7s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.912, total=  21.7s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.913, total=  20.9s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.904, total=  43.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.909, total=  48.6s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.912, total=  46.4s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................... C=100, score=0.903, total= 1.3min\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................... C=100, score=0.909, total= 1.4min\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ............................... C=100, score=0.911, total= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  9.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LinearSVC(dual=False, max_iter=5000),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]}, verbose=3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from grid search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, dual=False, max_iter=5000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9125595238095238"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "- Different representations\n",
    "- Kernels\n",
    "- Mercer's theorem\n",
    "- Gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel trick\n",
    "\n",
    "- What is the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVMs\n",
    "\n",
    "- How kernel SVMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial KSVM\n",
    "\n",
    "- What is the polynomial kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_params = {\"degree\": [2, 3],\n",
    "               \"C\": [0.1, 1, 10]}\n",
    "\n",
    "grid_poly = SVC(kernel='poly', coef0=1, max_iter=5000)\n",
    "\n",
    "poly_gridsearch = GridSearchCV(grid_poly, poly_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "poly_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from grid search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF) KSVM\n",
    "\n",
    "- What is the RBF kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_params = {\"gamma\": [1, 5, 10, 'scale', 'auto'],\n",
    "              \"C\": [0.1, 1, 10]}\n",
    "\n",
    "grid_rbf = SVC(kernel='rbf', dual=False, max_iter=5000)\n",
    "\n",
    "rbf_gridsearch = GridSearchCV(grid_rbf, rbf_params,\n",
    "                              cv=3, versbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid KSVM\n",
    "\n",
    "- What is the sigmoid kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_params = {\"gamma\": [1, 5, 10, 'scale', 'auto'],\n",
    "                  \"C\": [0.1, 1, 10]}\n",
    "\n",
    "grid_sigmoid = SVC(kernel='poly', coef0=1, max_iter=5000)\n",
    "\n",
    "sigmoid_gridsearch = GridSearchCV(grid_sigmoid, sigmoid_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of SVMs\n",
    "\n",
    "- Problems with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of ANNs and SVMs\n",
    "\n",
    "- Comparison of neural networks and support vector machines.\n",
    "- Area under ROC curve\n",
    "- Precision, recall, accuracy...\n",
    "- TP, TN, FP, FN and rates for each\n",
    "- Confusion matrix\n",
    "- Metric\n",
    "- Validation\n",
    "- Cross-validation\n",
    "- Prediction\n",
    "- Inference\n",
    "- Interpretability\n",
    "- Sensitivity vs specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
