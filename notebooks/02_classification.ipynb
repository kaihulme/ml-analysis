{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kai/anaconda3/envs/ml/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "- What is supervised learning?\n",
    "- Training\n",
    "- Fitting\n",
    "- Labels\n",
    "- Error / cost / objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "- What is classification?\n",
    "- Different approaches\n",
    "  - Logistic regression - log loss\n",
    "  - ANNs, SVMs\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No free lunch\n",
    "\n",
    "- Model selection.\n",
    "- Parametric / nonparametric models.\n",
    "- Discriminative / generative modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with MNIST\n",
    "\n",
    "- The aim of classification with MNIST\n",
    "- Get the data\n",
    "- Recap analysis\n",
    "  - Size, shape, type\n",
    "  - Features\n",
    "  - Missing data, outliers etc.\n",
    "  - Target feature of dataset (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, getting features X and labels y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall look at the number of unique features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 unique digit types.\n",
    "\n",
    "We shall put the data into a NumPy array and analyse the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784), y shape: (70000,)\n",
      "Image shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Image shape: {X[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape we can see that we have the full MNIST dataset of 70000 hand written didgets. \n",
    "\n",
    "They are yet to be split into a training and testing set and each image has been flattened to a (784,) array rather than the standard (28,28) form.\n",
    "\n",
    "Now we shall have a look at the image data to see what form it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X[0]), np.max(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the minimum and maximum values in appears that pixel are of the standard 0-255 intensity form. \n",
    "\n",
    "It is beneficial to some machine learning methods for values to be normalised between 0-1, notably neural networks so we shall do this in the data preparation step.\n",
    "\n",
    "Finally we shall check the datatypes for features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are of float type as expected but it may be beneficial to convert the string type labels to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "\n",
    "  - train, val test sets\n",
    "  - cross validation\n",
    "  - reshape / retype\n",
    "\n",
    "As stated earlier normalising the feature values will be beneficial for machine learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_full = X / 255.0\n",
    "print(np.min(X_full), np.max(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also now convert the string type features to integers for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "y_full = y.astype('int')\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create a train, validation and test set before we proceed using sklearns `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# remove a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "44800\n",
      "11200\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print(X_full.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the data and analyse using the validation set before testing on the test set. This will allow us to minimise overfitting by ensuring our model generalises well.\n",
    "\n",
    "Later on we shall use cross-validation on the entire training set to ensure maximum generalisation, but for now we shall use the the validation set for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "- Neurons structure\n",
    "- Action potentials ~ activation functions\n",
    "- Weights, connections etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "\n",
    "- Perceptron history\n",
    "- Input layer\n",
    "- Output layer\n",
    "- Activation function\n",
    "- Weights\n",
    "- Bias\n",
    "- Try and apply it to MNIST (lots of neurons?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "- Multiple layers of neurons\n",
    "- Able to learn more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPs\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Epochs, convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "- Calculation of outputs from input\n",
    "- Calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "- Gradient descent\n",
    "- Learning rate\n",
    "- Minimise loss function\n",
    "- Convex optimisation problem\n",
    "- Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- What is SGD\n",
    "- Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "- What is cross entropy\n",
    "- Sparse categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying MNIST\n",
    "\n",
    "- Feed forward network\n",
    "- Dense layers\n",
    "- Epochs, convergence\n",
    "- Create a simple MLP for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 4s 3ms/step - loss: 0.6903 - accuracy: 0.8213 - val_loss: 0.3466 - val_accuracy: 0.8991\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.3159 - accuracy: 0.9100 - val_loss: 0.2694 - val_accuracy: 0.9198\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.2600 - accuracy: 0.9252 - val_loss: 0.2393 - val_accuracy: 0.9301\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 4s 3ms/step - loss: 0.2237 - accuracy: 0.9356 - val_loss: 0.2081 - val_accuracy: 0.9381\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1956 - accuracy: 0.9433 - val_loss: 0.1880 - val_accuracy: 0.9433\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1738 - accuracy: 0.9501 - val_loss: 0.1700 - val_accuracy: 0.9488\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1560 - accuracy: 0.9548 - val_loss: 0.1567 - val_accuracy: 0.9526\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1417 - accuracy: 0.9592 - val_loss: 0.1450 - val_accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1291 - accuracy: 0.9632 - val_loss: 0.1383 - val_accuracy: 0.9596\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.1186 - accuracy: 0.9661 - val_loss: 0.1350 - val_accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper neural networks\n",
    "\n",
    "- Create a large overfitting network\n",
    "- Early stopping, check pointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation=\"relu\", input_shape=[784]))\n",
    "model.add(Dense(400, activation=\"relu\"))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(25, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 1.0119 - accuracy: 0.6736 - val_loss: 0.3504 - val_accuracy: 0.8922\n",
      "Epoch 2/20\n",
      "1400/1400 [==============================] - 8s 6ms/step - loss: 0.2486 - accuracy: 0.9279 - val_loss: 0.1961 - val_accuracy: 0.9433\n",
      "Epoch 3/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.1622 - accuracy: 0.9531 - val_loss: 0.1517 - val_accuracy: 0.9548\n",
      "Epoch 4/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.1213 - accuracy: 0.9643 - val_loss: 0.1370 - val_accuracy: 0.9588\n",
      "Epoch 5/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0954 - accuracy: 0.9718 - val_loss: 0.1221 - val_accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.0756 - accuracy: 0.9772 - val_loss: 0.1135 - val_accuracy: 0.9665\n",
      "Epoch 7/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.0596 - accuracy: 0.9823 - val_loss: 0.1034 - val_accuracy: 0.9703\n",
      "Epoch 8/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0481 - accuracy: 0.9851 - val_loss: 0.1013 - val_accuracy: 0.9703\n",
      "Epoch 9/20\n",
      "1400/1400 [==============================] - 9s 7ms/step - loss: 0.0379 - accuracy: 0.9888 - val_loss: 0.1006 - val_accuracy: 0.9725\n",
      "Epoch 10/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.1031 - val_accuracy: 0.9731\n",
      "Epoch 11/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0246 - accuracy: 0.9927 - val_loss: 0.1246 - val_accuracy: 0.9676\n",
      "Epoch 12/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.1557 - val_accuracy: 0.9602\n",
      "Epoch 13/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0145 - accuracy: 0.9963 - val_loss: 0.1182 - val_accuracy: 0.9707\n",
      "Epoch 14/20\n",
      "1400/1400 [==============================] - 13s 10ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.1062 - val_accuracy: 0.9754\n",
      "Epoch 15/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.1181 - val_accuracy: 0.9730\n",
      "Epoch 16/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.1109 - val_accuracy: 0.9765\n",
      "Epoch 17/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.1131 - val_accuracy: 0.9762\n",
      "Epoch 18/20\n",
      "1400/1400 [==============================] - 10s 7ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.1170 - val_accuracy: 0.9766\n",
      "Epoch 19/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.1358 - val_accuracy: 0.9706\n",
      "Epoch 20/20\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.1208 - val_accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / exploding gradient problem\n",
    "\n",
    "- What is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initlialisation\n",
    "\n",
    "- GloRoot initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturating activation functions\n",
    "\n",
    "- Leaky RELU\n",
    "- SELU\n",
    "- Tanh..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "- Batch normalisation\n",
    "- Mini-batch\n",
    "- Implement batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping\n",
    "\n",
    "- Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation of ANNs\n",
    "\n",
    "- Momentum\n",
    "- Adam\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in ANNs\n",
    "\n",
    "- Why large networks can overfit\n",
    "- l1, l2 loss and regularisation\n",
    "- Implement regularisation\n",
    "- Dropout, monte carlo dropout\n",
    "- Create a network with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks (CNN)\n",
    "\n",
    "- Problems with neural networks and images\n",
    "- Convolutions, filters\n",
    "- Convolutional layers, feature maps\n",
    "- Simple CNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training better CNNs\n",
    "\n",
    "- Stride, step\n",
    "- Pooling, max pooling\n",
    "- Better CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "- What is transfer learning\n",
    "- Use transfer learning with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of ANNs\n",
    "\n",
    "- Problems with ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines (SVMs)\n",
    "\n",
    "Train an SVM (with a chosen Kernel) and perform the same analyses as forANNs. Interpret  and  discuss  your  results. Does the model overfit? How do they compare with ANNs? And why? How does the type of kernel (e.g.linear, RBF, etc.) impact on performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines\n",
    "\n",
    "- Hyperplane\n",
    "- Support vectors\n",
    "- Minimising lagrange multiplier (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "- Implementation of SVM on MNIST\n",
    "- Plot decision boundaries from lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic SVM\n",
    "\n",
    "- What is a probabilistic SVM?\n",
    "- SVM on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dual problem\n",
    "\n",
    "What is the dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "- Different representations\n",
    "- Kernels\n",
    "- Mercer's theorem\n",
    "- Gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel trick\n",
    "\n",
    "- What is the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVMs\n",
    "\n",
    "- How kernel SVMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial KSVM\n",
    "\n",
    "- What is the polynomial kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF) KSVM\n",
    "\n",
    "- What is the RBF kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid KSVM\n",
    "\n",
    "- What is the sigmoid kernel\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising SVMs\n",
    "\n",
    "- Hinge loss\n",
    "- Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of SVMs\n",
    "\n",
    "- Problems with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of ANNs and SVMs\n",
    "\n",
    "- Comparison of neural networks and support vector machines.\n",
    "- Area under ROC curve\n",
    "- Precision, recall, accuracy...\n",
    "- TP, TN, FP, FN and rates for each\n",
    "- Confusion matrix\n",
    "- Metric\n",
    "- Validation\n",
    "- Cross-validation\n",
    "- Prediction\n",
    "- Inference\n",
    "- Interpretability\n",
    "- Sensitivity vs specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
